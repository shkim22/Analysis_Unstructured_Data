{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 5A #\n",
    "The purpose of this demonstration is reiterate topics covered in Lectures 5.1 and 5.2. After this demonstration, you should feel comfortable:\n",
    "- implementing an LDA model with `sklearn`\n",
    "- assessing the presence of topics in elements of the corpus\n",
    "- evaluating individual topics\n",
    "- computing model and topic evaluation metrics\n",
    "- tuning LDA models\n",
    "\n",
    "## The Data ##\n",
    "In the spirit of the example used in the lectures, we're going to use some complaints data to illustrate these models. I'm using data from the Consumer Financial Protection Bureau, which you can find __[here](https://www.consumerfinance.gov/data-research/consumer-complaints/#download-the-data)__. I've provided the compressed, JSON formatted file. If you inspect the text file, you'll see it is a list of JSON records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from zipfile import ZipFile\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ZipFile('complaints.json.zip','r') as z:\n",
    "    complaints = json.loads(z.read('complaints.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a sense as to what this data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6169994 complaints in this data.\n",
      "Sample Complaints:\n",
      "\n",
      "{'date_received': '2024-09-15', 'product': 'Credit reporting or other personal consumer reports', 'sub_product': 'Credit reporting', 'issue': 'Improper use of your report', 'sub_issue': 'Reporting company used your report improperly', 'complaint_what_happened': '', 'company_public_response': '', 'company': 'Paramount Recovery Systems, L.P.', 'state': 'FL', 'zip_code': '33578', 'tags': '', 'consumer_consent_provided': '', 'submitted_via': 'Web', 'date_sent_to_company': '2024-09-15', 'company_response': 'Closed with explanation', 'timely': 'Yes', 'consumer_disputed': 'N/A', 'complaint_id': '10131769'}\n",
      "{'date_received': '2024-07-22', 'product': 'Credit reporting or other personal consumer reports', 'sub_product': 'Credit reporting', 'issue': 'Incorrect information on your report', 'sub_issue': 'Information belongs to someone else', 'complaint_what_happened': '', 'company_public_response': '', 'company': 'Experian Information Solutions Inc.', 'state': 'FL', 'zip_code': '32837', 'tags': '', 'consumer_consent_provided': '', 'submitted_via': 'Web', 'date_sent_to_company': '2024-07-22', 'company_response': 'In progress', 'timely': 'Yes', 'consumer_disputed': 'N/A', 'complaint_id': '9572725'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(complaints)} complaints in this data.\")\n",
    "print(\"Sample Complaints:\\n\")\n",
    "print(complaints[0])\n",
    "print(complaints[1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual text of the complaint is in \"complaint_what_happened\". We'll create a dataframe and then retain only those complaints with that full text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2115520"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complaintsDF = pd.DataFrame(complaints)\n",
    "complaintsDF = complaintsDF.loc[complaintsDF['complaint_what_happened']!=\"\"]\n",
    "len(complaintsDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This culls about 2/3 of the data, but to keep this manageable let's pull a random sample of 10,000 complaints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "complaintsDF = complaintsDF.sample(10000,random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 10000 entries, 1144439 to 67420\n",
      "Data columns (total 18 columns):\n",
      " #   Column                     Non-Null Count  Dtype \n",
      "---  ------                     --------------  ----- \n",
      " 0   date_received              10000 non-null  object\n",
      " 1   product                    10000 non-null  object\n",
      " 2   sub_product                10000 non-null  object\n",
      " 3   issue                      10000 non-null  object\n",
      " 4   sub_issue                  10000 non-null  object\n",
      " 5   complaint_what_happened    10000 non-null  object\n",
      " 6   company_public_response    10000 non-null  object\n",
      " 7   company                    10000 non-null  object\n",
      " 8   state                      10000 non-null  object\n",
      " 9   zip_code                   10000 non-null  object\n",
      " 10  tags                       10000 non-null  object\n",
      " 11  consumer_consent_provided  10000 non-null  object\n",
      " 12  submitted_via              10000 non-null  object\n",
      " 13  date_sent_to_company       10000 non-null  object\n",
      " 14  company_response           10000 non-null  object\n",
      " 15  timely                     10000 non-null  object\n",
      " 16  consumer_disputed          10000 non-null  object\n",
      " 17  complaint_id               10000 non-null  object\n",
      "dtypes: object(18)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "complaintsDF.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the steps for running a topic model:\n",
    "1. Preprocessing\n",
    "2. Feature Extraction\n",
    "3. Model Selection\n",
    "4. Model Tuning & Evaluation\n",
    "5. Communicate Results\n",
    "\n",
    "We're going to use `sklearn` which allows us to combine some preprocessing steps within our feature extraction procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing & Feature Extraction ##\n",
    "We're going to generate a document term matrix with the following criteria:\n",
    "1. Include only letters and require at least 3 characters per token\n",
    "2. Remove stop words and dates (add XXXX and XX/XX/XXXX to stop words list)\n",
    "3. Consider bigrams or single words\n",
    "4. Retain only the 1,000 most common features\n",
    "\n",
    "As we've seen in previous demos, `sklearn` makes it very easy to incorporate these steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# load stop words\n",
    "stops = set(stopwords.words('english'))\n",
    "stops.add(\"xxxx\")\n",
    "stops.add(\"xx/xx/xxxx\")\n",
    "\n",
    "# set up vectorizer\n",
    "vec = CountVectorizer(token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                     stop_words = list(stops), # newer versions require stops to be list\n",
    "                     ngram_range = (1,2),\n",
    "                     max_features = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will generate our document-term matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = vec.fit_transform(complaintsDF['complaint_what_happened'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10000x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 440968 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most common words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "credit\n",
      "account\n",
      "information\n",
      "report\n",
      "consumer\n",
      "reporting\n",
      "credit report\n",
      "payment\n",
      "accounts\n",
      "section\n",
      "debt\n",
      "would\n",
      "also\n",
      "bank\n",
      "received\n",
      "company\n",
      "loan\n",
      "balance\n",
      "please\n",
      "card\n"
     ]
    }
   ],
   "source": [
    "for idx in reversed(dtm.todense().sum(axis=0).argsort()[:,-20:].tolist()[0]): # create dense matrix, compute word counts, use argsort, grab last 20 indices, convert to list (will be nested list, so take first element), pull feature names; reversed flips order\n",
    "    print(vec.get_feature_names_out()[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA with `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I've mentioned several times, scikit-learn has a very simple, intuitive API that makes it very easy to fit various models. The LDA model is available in `sklearn.decomposition`, and you can find the documentation for the model __[here](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html)__. Let's load the model and then we'll talk about the parameters we're going to set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation as LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you review the documentation, you'll find many different parameters we can set. We're going to set a few of them. In practice, you would likely want to \"tune\" many of these (try lots of different values and evaluate model fit). We're going to focus on a few:\n",
    "\n",
    "1. `n_components`: The number of topics (most important!)\n",
    "2. `doc_topic_prior`: This is \"alpha\" from the slides. The default value is 1/n_components (the number of topics). If you search around you'll find that, more typically, something like \"50/T\" is appropriate, where \"T\" is the number of topics. We will split the difference and go with 25/T\n",
    "3. `topic_word_prior`: This is \"beta\" from the slides. The default value is the same as for alpha, but if you look at other references many say to use something like 0.1 or 200/W, where W is the number of words (so 0.2 in our setting). We'll go with 0.15 to start.\n",
    "4. `n_jobs`: This allows you to parallelize. \"-1\" will use all cores.\n",
    "\n",
    "For simplicity, we'll leave the rest of the parameters the same. Go ahead and run this cell and come back when it's complete. It should take a couple of minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = 100\n",
    "\n",
    "lda = LDA(n_components = topics,\n",
    "         doc_topic_prior = 25/topics,\n",
    "         topic_word_prior = 0.15,\n",
    "         n_jobs = -1,\n",
    "         random_state=123)\n",
    "\n",
    "doc_topics = lda.fit_transform(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00185703, 0.00410383, 0.00186805, ..., 0.00196642, 0.15632024,\n",
       "        0.03958817],\n",
       "       [0.00463227, 0.00470617, 0.02292442, ..., 0.00477823, 0.00469649,\n",
       "        0.00474707],\n",
       "       [0.00741588, 0.00697513, 0.00696231, ..., 0.00699366, 0.00694445,\n",
       "        0.00699268],\n",
       "       ...,\n",
       "       [0.00265879, 0.00246359, 0.00252948, ..., 0.00265572, 0.00250008,\n",
       "        0.00261666],\n",
       "       [0.00574988, 0.0055684 , 0.00560237, ..., 0.02086514, 0.00560528,\n",
       "        0.14916491],\n",
       "       [0.00716393, 0.00698338, 0.00680025, ..., 0.00678511, 0.00678024,\n",
       "        0.04308231]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Exploring the Topic-Document Matrix \n",
    "Let's take a quick peek at our output; it should be 10,000 documents (our sample) by 100 columns (number of topics):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 100)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topics.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the topic-relevance scores are probabalistic and will sum to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topics.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that it is up to us to define a threshold for when a topic is actually relevant to a document. Let's set the threshold at 0.10 and examine (1) how many topics appear in each document, and (2) which topic identifiers are most common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 0.10\n",
    "dt2 = (doc_topics>threshold).astype(int)\n",
    "dt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5288"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (1) Topics per document:\n",
    "dt2.sum(axis=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([74, 62, 85, 41, 20, 91, 76, 44, 26, 24])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (2) topics that are most common:\n",
    "dt2.sum(axis=0).argsort()[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we know what constitutes topic 3, for instance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring Topics (with Topic-Word Matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One drawback of `sklearn`'s implementation of LDA, particularly as compared to `gensim`, is that it does not provide an easy way of accessing words relevant to each topic. However, we can use some simple numpy methods to access information in the topic-word matrix. We'll define a function to access this information in a moment, but let's start by exploring the underlying data we'll use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.18042992,  0.15050105,  0.15018962, ...,  0.15052758,\n",
       "         0.15001005,  0.15045665],\n",
       "       [ 0.17237487,  0.1503968 ,  0.15195612, ...,  0.55118859,\n",
       "         0.15000161,  0.1507837 ],\n",
       "       [ 1.36547275,  0.32566894,  0.15141976, ...,  0.15056611,\n",
       "         0.15000118,  4.32438008],\n",
       "       ...,\n",
       "       [ 0.15039791,  6.87093012,  0.15098516, ...,  0.73921355,\n",
       "         0.15002264, 33.47698058],\n",
       "       [ 0.15081336,  0.15048058,  0.15041882, ..., 23.45218189,\n",
       "         0.15000208,  3.62057035],\n",
       "       [ 0.15224601,  0.15046106,  8.47410813, ...,  0.15065494,\n",
       "         0.15000417,  0.15060303]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = np.array(vec.get_feature_names_out()) # the vocabulary of our DTM; storing as numpy array to make it easier to access multiple elements with a list (i.e., I can say vocab[[1,2,3,...]])\n",
    "top_word = lda.components_\n",
    "top_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matrix has dimensions 100 (topics) by 1000 (words). Higher values indicate a greater probability of the word being relevant for a given topic. So, to identify words that are most relevant for a given topic, we need to identify the highest values and use the column indices to access the vocabulary. Let's use topic 3 as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.67695435e+00, 1.50530984e-01, 1.50205662e-01, 1.50117855e-01,\n",
       "       2.76672271e-01, 1.50337960e-01, 1.66602764e-01, 1.78350628e+03,\n",
       "       6.06471907e+01, 1.50060296e-01, 1.50128787e-01, 1.50464774e-01,\n",
       "       1.24016543e+01, 1.12273048e+03, 1.50169151e-01, 1.50283446e-01,\n",
       "       1.50037922e-01, 1.50422996e-01, 3.26171423e+00, 1.50378173e-01,\n",
       "       1.50159955e-01, 1.50337012e-01, 1.50065352e-01, 1.50412377e-01,\n",
       "       1.50687703e-01, 1.50378494e-01, 4.00736406e-01, 1.50095817e-01,\n",
       "       1.50198760e-01, 1.50413416e-01, 1.50752270e-01, 1.50409709e-01,\n",
       "       1.50430854e-01, 1.50477363e-01, 1.50343440e-01, 1.50577894e-01,\n",
       "       1.50519635e-01, 1.50355922e-01, 1.50789015e-01, 1.50277251e-01,\n",
       "       3.00287711e-01, 1.50124492e-01, 1.52499488e-01, 1.50243491e-01,\n",
       "       1.50230199e-01, 1.50205797e-01, 7.16233378e-01, 1.50201448e-01,\n",
       "       1.50290407e-01, 1.50184088e-01, 1.50267691e-01, 1.50330138e-01,\n",
       "       1.50382150e-01, 1.50551862e-01, 1.50348236e-01, 1.50165187e-01,\n",
       "       1.50139023e-01, 1.50368394e-01, 1.50487366e-01, 1.50174120e-01,\n",
       "       1.50758227e-01, 1.50669633e-01, 8.86203028e+00, 1.50289593e-01,\n",
       "       1.50327599e-01, 1.50032749e-01, 1.50108395e-01, 1.67507822e-01,\n",
       "       1.50261242e-01, 1.50445568e-01, 1.50326582e-01, 1.50120286e-01,\n",
       "       1.50121044e-01, 1.50597641e-01, 1.50305020e-01, 1.50072187e-01,\n",
       "       1.50490600e-01, 1.50037022e-01, 1.50473978e-01, 1.50437875e-01,\n",
       "       1.50170512e-01, 1.50053117e-01, 1.50302537e-01, 1.50195578e-01,\n",
       "       2.10635941e+00, 1.50483956e-01, 1.50365418e-01, 1.50530602e-01,\n",
       "       1.61669196e+00, 1.50314272e-01, 1.50378187e-01, 1.50347373e-01,\n",
       "       1.50122797e-01, 1.50306150e-01, 1.50294399e-01, 1.50397826e-01,\n",
       "       1.50379725e-01, 1.50307446e-01, 1.50143639e-01, 1.50259959e-01,\n",
       "       1.50034368e-01, 1.50248772e-01, 1.50380294e-01, 1.50528894e-01,\n",
       "       1.50601554e-01, 1.50828618e-01, 1.50390047e-01, 1.50056731e-01,\n",
       "       1.50004848e-01, 1.50301138e-01, 1.50156042e-01, 1.50235802e-01,\n",
       "       1.50015505e-01, 1.50126489e-01, 1.50270185e-01, 1.50128494e-01,\n",
       "       1.50644801e-01, 1.50547369e-01, 1.07724061e+00, 1.50276059e-01,\n",
       "       2.12987946e+00, 1.50644820e-01, 1.50118816e-01, 1.50111861e-01,\n",
       "       1.50140957e-01, 1.50275665e-01, 1.50319543e-01, 1.50061498e-01,\n",
       "       1.50295857e-01, 1.50241720e-01, 1.50329889e-01, 1.50180280e-01,\n",
       "       1.50218860e-01, 1.50374434e-01, 1.50159836e-01, 1.50074449e-01,\n",
       "       1.50313901e-01, 1.50177354e-01, 1.50091618e-01, 1.50286374e-01,\n",
       "       1.50306257e-01, 1.50073061e-01, 1.50695815e-01, 1.50053833e-01,\n",
       "       1.50018636e-01, 1.50030896e-01, 1.50211575e-01, 1.50374420e-01,\n",
       "       1.50084170e-01, 1.51233967e-01, 1.51236066e-01, 1.50073344e-01,\n",
       "       1.50392516e-01, 1.50197422e-01, 1.50175270e-01, 1.50177302e-01,\n",
       "       1.50285010e-01, 1.50285493e-01, 1.50302311e-01, 1.50191891e-01,\n",
       "       1.50812069e-01, 1.50183937e-01, 1.50518513e-01, 1.50506172e-01,\n",
       "       1.50255112e-01, 1.50377125e-01, 1.50258348e-01, 1.50020592e-01,\n",
       "       1.50121706e-01, 1.50407346e-01, 1.50097810e-01, 1.50061054e-01,\n",
       "       1.50084752e-01, 1.50114899e-01, 1.50100476e-01, 1.50419964e-01,\n",
       "       1.50359288e-01, 1.50355103e-01, 1.50495780e-01, 1.50439132e-01,\n",
       "       1.50526123e-01, 1.50121981e-01, 1.50217683e-01, 1.50101024e-01,\n",
       "       1.50184456e-01, 1.50507888e-01, 1.50100397e-01, 1.58866664e-01,\n",
       "       1.50180076e-01, 1.50076574e-01, 1.50244183e-01, 1.50110923e-01,\n",
       "       1.50481707e-01, 1.50138619e-01, 1.50200837e-01, 1.50268091e-01,\n",
       "       1.50709234e-01, 1.50776545e-01, 1.50340179e-01, 1.50677147e-01,\n",
       "       1.50362089e-01, 1.50516509e-01, 1.50810809e-01, 1.50436823e-01,\n",
       "       1.50454651e-01, 1.50421844e-01, 1.50161554e-01, 1.50278213e-01,\n",
       "       1.50314871e-01, 1.50183196e-01, 1.50371969e-01, 1.50631205e-01,\n",
       "       1.50286638e-01, 1.50347934e-01, 1.50332648e-01, 5.26221528e+00,\n",
       "       1.50264053e-01, 1.50406804e-01, 1.50166734e-01, 1.50339833e-01,\n",
       "       1.50795858e-01, 1.50129233e-01, 1.50357161e-01, 1.50330072e-01,\n",
       "       3.56824640e+00, 1.50449464e-01, 1.50220433e-01, 1.50265594e-01,\n",
       "       1.50122285e-01, 1.50564848e-01, 1.50291062e-01, 1.50167287e-01,\n",
       "       1.50146660e-01, 1.50119185e-01, 1.50123254e-01, 1.50255870e-01,\n",
       "       4.66701650e-01, 2.25588483e-01, 1.50491702e-01, 1.50473406e-01,\n",
       "       1.50330110e-01, 1.50200499e-01, 1.50462702e-01, 1.50165725e-01,\n",
       "       2.69595749e+01, 1.50164016e-01, 1.50217120e-01, 1.50144865e-01,\n",
       "       1.50079431e-01, 1.50151922e-01, 1.50055363e-01, 1.50163842e-01,\n",
       "       3.48109378e+00, 5.65569138e-01, 1.50128222e-01, 1.50071538e-01,\n",
       "       9.07251942e-01, 1.50405002e-01, 1.50250640e-01, 1.50121432e-01,\n",
       "       1.52689241e-01, 1.50388403e-01, 1.50146244e-01, 1.50066834e-01,\n",
       "       1.50170591e-01, 1.53665959e-01, 1.50251847e-01, 8.13487314e-01,\n",
       "       1.50107120e-01, 1.50232831e-01, 2.95616717e-01, 1.50465174e-01,\n",
       "       1.50340072e-01, 1.51466066e-01, 1.50419759e-01, 1.50064137e-01,\n",
       "       1.50080659e-01, 1.50034894e-01, 1.50225772e-01, 1.50056154e-01,\n",
       "       1.50080693e-01, 1.50345672e-01, 1.50143976e-01, 1.50349380e-01,\n",
       "       4.61933014e-01, 1.50379838e-01, 1.50553038e-01, 1.50105028e-01,\n",
       "       1.50188578e-01, 1.50184190e-01, 1.50794948e-01, 1.50203477e-01,\n",
       "       1.50786432e-01, 1.50301236e-01, 1.50099386e-01, 1.50584613e-01,\n",
       "       1.50456466e-01, 1.50238958e-01, 2.82737313e+00, 1.50267468e-01,\n",
       "       1.50664512e-01, 1.50368661e-01, 1.50467074e-01, 1.50218444e-01,\n",
       "       7.52312281e-01, 1.81610205e+00, 1.55081526e-01, 1.50790164e-01,\n",
       "       1.50338636e-01, 1.50283735e-01, 1.50341602e-01, 1.50087017e-01,\n",
       "       1.50261737e-01, 1.50285491e-01, 1.50426606e-01, 1.50494352e-01,\n",
       "       1.50421414e-01, 1.50149348e-01, 1.50066703e-01, 1.50341156e-01,\n",
       "       1.50297383e-01, 1.50154292e-01, 1.14470680e+00, 1.50396828e-01,\n",
       "       1.50218822e-01, 1.50179970e-01, 1.50279637e-01, 1.50406805e-01,\n",
       "       1.50030022e-01, 1.50500166e-01, 1.50365195e-01, 1.50130348e-01,\n",
       "       1.50592176e-01, 1.50485993e-01, 1.50026928e-01, 1.50319935e-01,\n",
       "       1.50292264e-01, 1.50215168e-01, 1.50109118e-01, 1.50344716e-01,\n",
       "       1.50221870e-01, 1.50224769e-01, 1.50246396e-01, 1.51073414e-01,\n",
       "       1.50229733e-01, 5.84928529e-01, 1.50478550e-01, 3.14317582e-01,\n",
       "       1.93498510e-01, 1.50067681e-01, 1.50611590e-01, 1.50282996e-01,\n",
       "       1.50191116e-01, 1.50006131e-01, 1.50534900e-01, 1.50115997e-01,\n",
       "       1.50397931e-01, 1.50295371e-01, 1.50219195e-01, 1.50083923e-01,\n",
       "       1.50326350e-01, 1.50318004e-01, 1.50063012e-01, 1.50437273e-01,\n",
       "       1.50123587e-01, 1.50461243e-01, 1.50081603e-01, 1.50472071e-01,\n",
       "       1.50073080e-01, 1.50286013e-01, 1.50044391e-01, 1.50261166e-01,\n",
       "       1.50288684e-01, 1.50167926e-01, 1.50252240e-01, 1.50053071e-01,\n",
       "       1.50140373e-01, 1.50434960e-01, 1.50392243e-01, 1.50316570e-01,\n",
       "       1.50153987e-01, 1.50355397e-01, 4.28942723e+01, 3.02674624e+00,\n",
       "       1.50618521e-01, 1.50040655e-01, 1.50022507e-01, 1.50277523e-01,\n",
       "       1.50318395e-01, 1.50516169e-01, 1.50582733e-01, 1.50348167e-01,\n",
       "       1.50476421e-01, 1.50214243e-01, 1.50150476e-01, 1.50085788e-01,\n",
       "       1.50437387e-01, 1.62302165e-01, 1.50218630e-01, 1.50097746e-01,\n",
       "       1.50299090e-01, 1.50201068e-01, 1.50199165e-01, 1.50430556e-01,\n",
       "       1.50286801e-01, 1.50525857e-01, 1.50302054e-01, 1.50390599e-01,\n",
       "       1.50379965e-01, 1.50362399e-01, 1.50275421e-01, 2.46183078e-01,\n",
       "       1.50539698e-01, 1.50475961e-01, 1.50253802e-01, 1.50237208e-01,\n",
       "       1.50034784e-01, 1.50036683e-01, 1.50474671e-01, 1.50091227e-01,\n",
       "       1.50357190e-01, 1.50312292e-01, 1.50320447e-01, 1.50262041e-01,\n",
       "       1.50468102e-01, 1.50184379e-01, 1.50314205e-01, 1.50141777e-01,\n",
       "       1.50128444e-01, 1.50205915e-01, 1.50191765e-01, 1.50377430e-01,\n",
       "       1.50109539e-01, 1.50457641e-01, 1.50272883e-01, 1.50190476e-01,\n",
       "       1.50221472e-01, 1.50261955e-01, 1.50352773e-01, 1.64458647e-01,\n",
       "       1.60167042e-01, 7.66898264e+00, 1.50505949e-01, 1.50143677e-01,\n",
       "       1.50505627e-01, 1.50751841e-01, 1.50660727e-01, 1.50103374e-01,\n",
       "       1.50738095e-01, 1.50445461e-01, 1.50123677e-01, 1.50062681e-01,\n",
       "       1.50346508e-01, 1.50332863e-01, 1.50292138e-01, 1.50560472e-01,\n",
       "       3.31735854e+01, 1.50124187e-01, 1.50151175e-01, 1.50545049e-01,\n",
       "       1.50307803e-01, 1.50227337e-01, 1.50281768e-01, 1.50218285e-01,\n",
       "       1.50479031e-01, 1.50343761e-01, 1.50312686e-01, 1.01583733e+00,\n",
       "       1.50131154e-01, 1.50043776e-01, 1.50055046e-01, 1.50279654e-01,\n",
       "       1.50123299e-01, 1.61666681e-01, 1.50142270e-01, 1.50178791e-01,\n",
       "       1.50091618e-01, 1.50108656e-01, 1.50395981e-01, 1.50700024e-01,\n",
       "       1.50690500e-01, 1.50265751e-01, 1.50325282e-01, 1.50442554e-01,\n",
       "       1.50589738e-01, 4.41922714e+00, 1.50218069e-01, 1.77686619e+00,\n",
       "       1.50350842e-01, 1.50189417e-01, 1.50328570e-01, 1.50184674e-01,\n",
       "       1.50290032e-01, 1.50488623e-01, 1.50079380e-01, 1.50558079e-01,\n",
       "       1.50439262e-01, 1.50368658e-01, 1.50177206e-01, 1.50163901e-01,\n",
       "       1.51323778e-01, 1.50183966e-01, 1.50090634e-01, 1.50277667e-01,\n",
       "       1.50242502e-01, 1.50275471e-01, 1.50507635e-01, 1.50636427e-01,\n",
       "       1.50448744e-01, 1.50306510e-01, 1.50347037e-01, 1.50099463e-01,\n",
       "       1.50298498e-01, 1.50195985e-01, 1.50249519e-01, 1.50590176e-01,\n",
       "       1.50434098e-01, 1.50241361e-01, 1.50413697e-01, 1.50447633e-01,\n",
       "       1.50230532e-01, 1.50418029e-01, 1.59842063e-01, 1.50424109e-01,\n",
       "       1.52903545e-01, 1.50088582e-01, 1.50246335e-01, 1.50459780e-01,\n",
       "       1.50109657e-01, 1.50092450e-01, 1.50500923e-01, 1.50524580e-01,\n",
       "       1.50547140e-01, 1.50339615e-01, 1.50136364e-01, 1.50242983e-01,\n",
       "       1.50407221e-01, 1.50527111e-01, 1.50378749e-01, 1.50376572e-01,\n",
       "       1.50509154e-01, 1.50120633e-01, 1.50381032e-01, 1.50226342e-01,\n",
       "       1.50168798e-01, 1.50250186e-01, 1.50640397e-01, 1.50648119e-01,\n",
       "       2.67617040e+00, 1.50120297e-01, 1.50426194e-01, 1.50118872e-01,\n",
       "       1.50317119e-01, 1.50053642e-01, 1.50344416e-01, 1.50437512e-01,\n",
       "       1.58451493e-01, 1.50375985e-01, 1.50042656e-01, 1.50132294e-01,\n",
       "       1.50015688e-01, 1.50210275e-01, 1.50194984e-01, 1.50147608e-01,\n",
       "       1.50154020e-01, 1.50539756e-01, 1.50054870e-01, 1.50027626e-01,\n",
       "       1.50305389e-01, 1.50511268e-01, 1.50071847e-01, 1.50790151e-01,\n",
       "       1.50502389e-01, 9.37340828e+01, 1.56518243e+01, 1.66894517e-01,\n",
       "       1.50093998e-01, 1.50291303e-01, 1.50879470e-01, 1.50201034e-01,\n",
       "       2.27699808e+01, 1.50501860e-01, 1.50383259e-01, 1.50115019e-01,\n",
       "       1.50122360e-01, 1.50272147e-01, 1.50607387e-01, 1.50461259e-01,\n",
       "       1.50509146e-01, 1.50731775e-01, 1.50525449e-01, 1.50368744e-01,\n",
       "       1.50238297e-01, 1.50652098e-01, 1.50342308e-01, 1.50291712e-01,\n",
       "       1.50514758e-01, 1.79401613e+03, 3.14499625e+02, 3.35926775e-01,\n",
       "       1.50395254e-01, 1.69905878e-01, 1.50208763e-01, 4.19375544e-01,\n",
       "       1.50265230e-01, 1.50171173e-01, 1.50146011e-01, 1.62631684e-01,\n",
       "       1.50283321e-01, 1.50240280e-01, 1.50425420e-01, 9.34187579e-01,\n",
       "       1.50044796e-01, 3.19350342e+00, 1.50291540e-01, 1.50582956e-01,\n",
       "       1.50219021e-01, 1.50290297e-01, 1.50150052e-01, 1.50269869e-01,\n",
       "       1.50129955e-01, 1.50073649e-01, 1.50240957e-01, 1.50037564e-01,\n",
       "       1.50550349e-01, 1.50123890e-01, 1.50294972e-01, 1.50381931e-01,\n",
       "       1.50197189e-01, 1.50481501e-01, 1.50196738e-01, 1.50291790e-01,\n",
       "       1.50286287e-01, 1.50399927e-01, 1.50061586e-01, 1.50076852e-01,\n",
       "       1.50274511e-01, 1.50275827e-01, 1.50017273e-01, 1.50195467e-01,\n",
       "       1.50806334e-01, 1.50550817e-01, 1.50513937e-01, 1.50387995e-01,\n",
       "       5.77359133e-01, 1.50161828e-01, 1.50256196e-01, 1.50455951e-01,\n",
       "       1.50337730e-01, 1.50248641e-01, 1.50074461e-01, 1.50035293e-01,\n",
       "       1.50573799e-01, 1.50205630e-01, 1.50726627e-01, 1.50135826e-01,\n",
       "       2.03226008e-01, 1.50359619e-01, 1.50048259e-01, 1.50487323e-01,\n",
       "       1.50160794e-01, 1.50500378e-01, 1.50299391e-01, 1.50186838e-01,\n",
       "       1.50576281e-01, 1.50213144e-01, 1.50147013e-01, 1.50626859e-01,\n",
       "       1.50550651e-01, 1.50285964e-01, 1.50908442e-01, 1.50462603e-01,\n",
       "       1.50064387e-01, 1.50280876e-01, 1.52279733e-01, 1.50209607e-01,\n",
       "       1.50237019e-01, 1.50086541e-01, 1.50394403e-01, 1.50273321e-01,\n",
       "       1.50473960e-01, 1.50200651e-01, 1.50063198e-01, 1.50165642e-01,\n",
       "       1.50565619e-01, 1.50355038e-01, 1.50251459e-01, 1.50026773e-01,\n",
       "       1.50478899e-01, 1.50312252e-01, 1.50304519e-01, 1.50203860e-01,\n",
       "       1.50126019e-01, 1.50687643e-01, 1.50507097e-01, 1.50759220e-01,\n",
       "       1.50324183e-01, 1.50686666e-01, 1.50263457e-01, 1.50313967e-01,\n",
       "       1.50568232e-01, 1.50472291e-01, 1.50289177e-01, 1.50484739e-01,\n",
       "       1.50901579e-01, 1.50187554e-01, 1.50078631e-01, 1.50163506e-01,\n",
       "       1.50478867e-01, 1.50106934e-01, 2.85054404e-01, 1.50335714e-01,\n",
       "       1.50198440e-01, 1.50901453e-01, 1.50400728e-01, 1.50506439e-01,\n",
       "       1.50084094e-01, 1.50077947e-01, 1.50767623e-01, 1.50329391e-01,\n",
       "       1.50440705e-01, 1.50591599e-01, 1.50631500e-01, 1.50490035e-01,\n",
       "       1.50122601e-01, 1.50197851e-01, 1.52135496e-01, 1.50348704e-01,\n",
       "       1.50091956e-01, 1.50261129e-01, 1.50450865e-01, 3.97185139e-01,\n",
       "       1.50371879e-01, 4.22626492e-01, 1.50373641e-01, 1.53856741e-01,\n",
       "       1.50211570e-01, 1.50263551e-01, 1.50304781e-01, 1.50313924e-01,\n",
       "       1.50557766e-01, 1.50344757e-01, 1.50292001e-01, 1.50060667e-01,\n",
       "       1.09563260e+01, 1.50157706e-01, 1.50145373e-01, 1.50119128e-01,\n",
       "       1.50122711e-01, 1.28747455e+01, 1.50295137e-01, 1.14758660e+01,\n",
       "       1.50204489e-01, 3.35578178e-01, 1.50173589e-01, 1.50413190e-01,\n",
       "       1.57092851e-01, 1.50366129e-01, 1.50258977e-01, 1.50268710e-01,\n",
       "       1.50330866e-01, 1.50091900e-01, 1.78564024e-01, 1.50416922e-01,\n",
       "       1.50686782e-01, 1.50209158e-01, 1.50315756e-01, 1.50493423e-01,\n",
       "       1.50157284e-01, 1.50547264e-01, 1.50358876e-01, 1.50891920e-01,\n",
       "       1.50115850e-01, 1.50519981e-01, 1.50465115e-01, 1.50432694e-01,\n",
       "       1.50278103e-01, 1.50386950e-01, 1.50475273e-01, 1.50389167e-01,\n",
       "       1.50265295e-01, 1.50235694e-01, 1.50301018e-01, 1.50458643e-01,\n",
       "       1.50287561e-01, 1.50577279e-01, 1.69199306e+00, 1.50332757e-01,\n",
       "       1.51385905e-01, 1.50284008e-01, 1.50077903e-01, 1.50170065e-01,\n",
       "       1.50217523e-01, 1.50100057e-01, 1.50035173e-01, 1.50056642e-01,\n",
       "       1.50274640e-01, 1.50389846e-01, 1.50194631e-01, 1.50511413e-01,\n",
       "       1.50080124e-01, 1.50772604e-01, 8.48800846e-01, 1.50220661e-01,\n",
       "       1.50241151e-01, 1.50328095e-01, 1.50042472e-01, 1.50094228e-01,\n",
       "       1.50079729e-01, 1.50052578e-01, 1.50371842e-01, 1.50013349e-01,\n",
       "       1.50225831e-01, 1.50303341e-01, 1.50774826e-01, 3.86024109e-01,\n",
       "       1.50411622e-01, 1.50163153e-01, 1.50223113e-01, 1.50055812e-01,\n",
       "       1.50157448e-01, 1.50076159e-01, 1.50266522e-01, 1.50600053e-01,\n",
       "       1.50648786e-01, 1.50618838e-01, 1.50439996e-01, 1.50062917e-01,\n",
       "       1.50450726e-01, 1.50252780e-01, 1.50239899e-01, 1.50324796e-01,\n",
       "       1.50331189e-01, 1.50387613e-01, 1.50237809e-01, 1.50243277e-01,\n",
       "       1.50386618e-01, 1.50719959e-01, 1.52529748e-01, 1.50346407e-01,\n",
       "       1.50280730e-01, 1.50058001e-01, 1.50052807e-01, 1.50392704e-01,\n",
       "       1.50380349e-01, 1.50489873e-01, 1.50407984e-01, 1.50221883e-01,\n",
       "       1.50121637e-01, 1.50337537e-01, 1.50355247e-01, 1.50404311e-01,\n",
       "       1.50395288e-01, 1.50204735e-01, 1.50220112e-01, 1.50340629e-01,\n",
       "       1.50426122e-01, 1.50375682e-01, 1.50416563e-01, 1.53852742e+00,\n",
       "       1.50274044e-01, 1.50239813e-01, 1.50450841e-01, 2.07523961e+00,\n",
       "       2.03856582e-01, 1.50140342e-01, 1.50523318e-01, 1.50088503e-01,\n",
       "       1.50033573e-01, 1.50045119e-01, 4.38636702e-01, 1.50216862e-01,\n",
       "       1.73214707e-01, 1.50339116e-01, 1.50212262e-01, 1.50117496e-01,\n",
       "       1.50356295e-01, 1.50672907e-01, 1.50871735e-01, 1.50452248e-01,\n",
       "       1.50783614e-01, 1.50603483e-01, 1.50248781e-01, 1.50950540e-01,\n",
       "       1.50134637e-01, 1.50066989e-01, 1.50392459e-01, 1.50503985e-01,\n",
       "       1.50253024e-01, 1.50247589e-01, 1.50252599e-01, 1.50738309e-01,\n",
       "       1.50260819e-01, 1.50152817e-01, 1.50422308e-01, 1.50727347e-01,\n",
       "       1.50421242e-01, 1.50175851e-01, 1.50252289e-01, 1.50117531e-01,\n",
       "       1.50164547e-01, 1.50143683e-01, 1.50235955e-01, 1.50104584e-01,\n",
       "       1.50269818e-01, 1.50597307e-01, 1.50349685e-01, 1.50315282e-01,\n",
       "       1.50112808e-01, 1.50416058e-01, 1.50232452e-01, 1.50080261e-01,\n",
       "       1.50200171e-01, 1.50256577e-01, 1.50094188e-01, 1.50096560e-01,\n",
       "       1.50136638e-01, 1.50221943e-01, 1.50205246e-01, 1.50251938e-01,\n",
       "       1.50210770e-01, 1.50119961e-01, 1.50041525e-01, 1.50260723e-01,\n",
       "       1.50662419e-01, 1.50325535e-01, 1.50629812e-01, 1.50437133e-01,\n",
       "       1.50152531e-01, 1.50367067e-01, 1.97648708e-01, 1.50451159e-01,\n",
       "       1.50391445e-01, 1.50886180e-01, 1.50262343e-01, 1.50101870e-01,\n",
       "       1.50077299e-01, 1.50082220e-01, 1.50377021e-01, 1.50316056e-01,\n",
       "       1.50167824e-01, 1.50248330e-01, 3.68104877e+00, 1.50458088e-01,\n",
       "       1.50391902e-01, 1.50031328e-01, 1.50289923e-01, 1.50551190e-01,\n",
       "       1.50463310e-01, 1.50473689e-01, 1.50314595e-01, 1.50568932e-01,\n",
       "       1.50289460e-01, 1.50137011e-01, 1.50395884e-01, 1.50444695e-01,\n",
       "       2.28961068e+00, 1.50725279e-01, 1.50349844e-01, 1.50184891e-01,\n",
       "       1.51080321e-01, 1.50212776e-01, 1.50205822e-01, 1.50079191e-01,\n",
       "       1.50376781e-01, 1.50500793e-01, 1.50189608e-01, 1.50800138e-01,\n",
       "       1.50348773e-01, 1.50898063e-01, 1.50201441e-01, 1.50128925e-01,\n",
       "       1.50206619e-01, 1.50512402e-01, 1.50006271e-01, 1.50006203e-01,\n",
       "       1.50199221e-01, 1.50071905e-01, 1.50499209e-01, 1.50360092e-01,\n",
       "       9.45716865e-01, 1.50095171e-01, 1.50313431e-01, 1.50323955e-01,\n",
       "       1.50128296e-01, 1.50456715e-01, 1.50263116e-01, 1.50530330e-01,\n",
       "       3.16034692e-01, 1.50090967e-01, 1.50276755e-01, 1.50297206e-01,\n",
       "       1.50346789e-01, 9.59215119e+00, 1.50026983e-01, 1.50215907e-01,\n",
       "       5.45835362e-01, 1.25154760e+00, 1.50132792e-01, 1.50700265e-01])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic = 3\n",
    "# the word vector for that topic:\n",
    "top_word[3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 most relevant words (from most to least relevant with [::-1])\n",
    "top_words = top_word[3,:].argsort()[-20:][::-1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[597,\n",
       " 7,\n",
       " 13,\n",
       " 598,\n",
       " 573,\n",
       " 8,\n",
       " 382,\n",
       " 456,\n",
       " 244,\n",
       " 580,\n",
       " 574,\n",
       " 749,\n",
       " 12,\n",
       " 751,\n",
       " 744,\n",
       " 993,\n",
       " 62,\n",
       " 441,\n",
       " 215,\n",
       " 485]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use this top_words item to access the words in the vocabulary corresponding to this topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['number', 'account', 'account number', 'number account', 'name',\n",
       "       'account account', 'following', 'information', 'credit', 'needs',\n",
       "       'name account', 'reported', 'account name', 'reporting', 'report',\n",
       "       'xxxxxxxx', 'also', 'inaccurate', 'consumer', 'item'], dtype=object)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[top_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sounds like a coherent topic relating to bank accounts, and more specifically transferring funds or closing accounts.\n",
    "\n",
    "Let's now transfer this code to a function we can use to access the words for any topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['number', 'account', 'account number', 'number account', 'name'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_topic_words(topic,top_word,vocab,topn=10):\n",
    "    top_words = top_word[topic,:].argsort()[-topn:][::-1].tolist()\n",
    "    return vocab[top_words]\n",
    "\n",
    "# Test:\n",
    "get_topic_words(3,top_word,vocab,topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's iterate over the topics and print out the 5 most common words for each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: accurate|information|update|credit|investigate\n",
      "Topic 1: attached|see|investigation|want|inc\n",
      "Topic 2: business|days|business days|within|complaint\n",
      "Topic 3: number|account|account number|number account|name\n",
      "Topic 4: data|caused|breach|affected|continues\n",
      "Topic 5: account|access|branch|opened|information\n",
      "Topic 6: money|back|said|get|hold\n",
      "Topic 7: security|social|social security|number|name\n",
      "Topic 8: bureaus|credit bureaus|multiple|credit|multiple times\n",
      "Topic 9: experian|local|behalf|also|credit\n",
      "Topic 10: inquiry|authorized|account|remove credit|credit report\n",
      "Topic 11: information|credit|creditors|properly|inaccurate\n",
      "Topic 12: bank|like|would|would like|bank account\n",
      "Topic 13: usc|information|inaccurate|consumer|according\n",
      "Topic 14: legal|items|action|legal action|take\n",
      "Topic 15: fraudulent|credit|report|possible|accounts\n",
      "Topic 16: account|incorrect|information|account account|incorrect information\n",
      "Topic 17: said|asked|would|stating|letter\n",
      "Topic 18: court|debt|collections|show|collect\n",
      "Topic 19: help|causing|credit|please help|please\n",
      "Topic 20: section|states|account|reporting|without\n",
      "Topic 21: equifax|information|verified|documentation|report\n",
      "Topic 22: remove|please|please remove|mine|items\n",
      "Topic 23: due|amount|statement|past|statements\n",
      "Topic 24: told|would|called|back|could\n",
      "Topic 25: complaint|party|third|letters|filing\n",
      "Topic 26: even|years|year|though|time\n",
      "Topic 27: account|reporting|reported|report|history\n",
      "Topic 28: rights|violated|violation|consumer rights|violating\n",
      "Topic 29: several|fraud|companies|times|submitted\n",
      "Topic 30: bureau|financial|protection|credit bureau|agency\n",
      "Topic 31: name|account|account name|name account|account account\n",
      "Topic 32: consumer|information|reporting|agency|consumer reporting\n",
      "Topic 33: balance|opened|dept|status|date\n",
      "Topic 34: means|plan|credit|term|person\n",
      "Topic 35: bank|america|bank america|account|claim\n",
      "Topic 36: account|checking|checking account|transfer|savings\n",
      "Topic 37: request|matter|credit|attention|writing\n",
      "Topic 38: payment|escrow|taxes|late|tax\n",
      "Topic 39: cash|refund|form|app|deposit\n",
      "Topic 40: application|income|approved|recent|apply\n",
      "Topic 41: report|credit|credit report|info|get\n",
      "Topic 42: report|theft|information|items|identity\n",
      "Topic 43: xxxxxxxx|xxxxxxxx xxxxxxxx|balance|records|xxxxxxxx balance\n",
      "Topic 44: credit|information|report|credit report|reporting\n",
      "Topic 45: debt|collection|collector|debt collector|debt collection\n",
      "Topic 46: information|consumer|usc|financial|personal\n",
      "Topic 47: car|medical|vehicle|sold|finance\n",
      "Topic 48: closed|account|account closed|closed account|change\n",
      "Topic 49: email|order|received|emails|via\n",
      "Topic 50: owed|balance|acct|balance owed|opened balance\n",
      "Topic 51: original|creditor|contract|original creditor|signature\n",
      "Topic 52: loans|loan|student|navient|student loan\n",
      "Topic 53: one|capital|capital one|like|would\n",
      "Topic 54: reports|transunion|credit reports|credit|vehicle\n",
      "Topic 55: letter|sent|received|letters|stating\n",
      "Topic 56: foreclosure|title|sale|modification|review\n",
      "Topic 57: date|inquiry|inquiry date|date inquiry|date last\n",
      "Topic 58: company|never|owe|know|american\n",
      "Topic 59: accounts|listed|accounts credit|accounts listed|credit\n",
      "Topic 60: insurance|policy|citibank|dont|escrow\n",
      "Topic 61: chase|card|debit|fraud|citi\n",
      "Topic 62: consumer|reporting|information|section|consumer reporting\n",
      "Topic 63: payment|made|payments|late payment|time\n",
      "Topic 64: consent|without|reports|consumer|gave\n",
      "Topic 65: consumer|report|consumer report|information|agency\n",
      "Topic 66: states|state|office|united|federal\n",
      "Topic 67: card|credit|credit card|card account|applied\n",
      "Topic 68: loan|auto|agreement|amount|finance\n",
      "Topic 69: reporting|credit|credit reporting|agencies|reporting agencies\n",
      "Topic 70: never|removed|response|received|credit\n",
      "Topic 71: check|money|funds|account|checks\n",
      "Topic 72: credit|score|credit score|cards|points\n",
      "Topic 73: credit|open|usc|consumer|creditor\n",
      "Topic 74: payments|pay|month|payment|monthly\n",
      "Topic 75: bill|online|get|pay|tried\n",
      "Topic 76: identity|theft|identity theft|victim|information\n",
      "Topic 77: notice|error|billing|code|aware\n",
      "Topic 78: service|customer|customer service|amount|issue\n",
      "Topic 79: paid|full|paid full|still|credit\n",
      "Topic 80: claim|transactions|two|merchant|paypal\n",
      "Topic 81: fees|charged|fee|charges|interest\n",
      "Topic 82: account|credit|disputed|account credit|three\n",
      "Topic 83: mortgage|closing|company|servicing|mortgage company\n",
      "Topic 84: address|dispute|item|addresses|current\n",
      "Topic 85: credit|report|credit report|removed|collection\n",
      "Topic 86: late|payments|late payments|days|days late\n",
      "Topic 87: days|disputes|items|deleted|credit\n",
      "Topic 88: phone|call|calls|number|contact\n",
      "Topic 89: charge|transaction|account|related|dispute\n",
      "Topic 90: act|fair|fair credit|credit|reporting act\n",
      "Topic 91: credit|inquiries|report|credit report|unauthorized\n",
      "Topic 92: home|property|loan|also|mortgage\n",
      "Topic 93: fix|get|help|fixed|problem\n",
      "Topic 94: services|place|returned|authorize|cancelled\n",
      "Topic 95: alleged|proof|validation|debt|claim\n",
      "Topic 96: delete|police|report|trade|federal\n",
      "Topic 97: file|bankruptcy|credit file|filed|court\n",
      "Topic 98: fargo|wells|wells fargo|issue|complaint\n",
      "Topic 99: fcra|law|section|required|violation\n"
     ]
    }
   ],
   "source": [
    "for top in range(topics):\n",
    "    words = get_topic_words(top,top_word,vocab,topn=5)\n",
    "    print(f\"Topic {top}: {'|'.join(words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is subjective, but overall this seems to have done a pretty good job! Now let's explore how we might evaluate model and topic quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Model & Topic Quality\n",
    "In the lectures, we talked about several different approaches to evaluating model quality. At the model level, we talked about perplexity. We compute perplexity as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity = lda.perplexity(dtm)\n",
    "perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This really isn't interpretable on its own--it's meant to be compared to other models (or used in the elbow method). We'll come back to this later (there's actually a bug in `sklearn`'s perplexity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for evaluating the quality of the topics, there are numerous metrics out there but none are built into `sklearn` directory. The other main LDA implementation in Python, available through `gensim`, does have many different metrics available. While we cannot use `gensim` methods and functions on `sklearn` objects, there is a package available that provides a link between the two, `tmtoolkit`. We'll compute one metric, called \"u_mass\", which is one of the indices that evaluates the likelihood words in a topic co-occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tmtoolkit.topicmod.evaluate import metric_coherence_gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umass = metric_coherence_gensim(measure = \"u_mass\", # essentially a measure of how likely a word pair in a topic is in a corpus, higher is better (but will be negative)\n",
    "                        top_n = 5,\n",
    "                        topic_word_distrib = top_word,\n",
    "                        dtm = dtm.todense(),\n",
    "                        vocab = vocab,\n",
    "                        texts = None) # note that metrics other than \"u_mass\" require texts to be passed as tokenized text in a list of lists (because they use sliding windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(umass[:10]) # by topic\n",
    "print(np.mean(umass))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These metrics are relatively easy to calculate, but they they can sometimes suggest a different level of quality than what a human would interpret. Word intrusion tasks can be very helpful for evaluating topic quality.\n",
    "\n",
    "Let's set up a word intrusion task by doing the following:\n",
    "(1) Collect the top 5 words associated with each topic\n",
    "(2) Add a random word from the rest of the vocabulary\n",
    "\n",
    "In practice, we'd then randomize the order of the 5 words, but we won't do that so we can evaluate the intruder vs. other words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.sample(list(vocab),1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "for top in range(topics):\n",
    "    words = get_topic_words(top,top_word,vocab,topn=5)\n",
    "    random_word = random.sample(list([w for w in vocab if w not in words]),1)[0]\n",
    "    print(f\"Topic {top} words: {'|'.join(words)}\\t\\tIntruder: {random_word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, it seems like the intruders are pretty clearly different than the other topic words with few exceptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the model ###\n",
    "The last thing we will cover (briefly) is how you might go about tuning this LDA model. In practice, you would likely want to try a variety of different parameters, and `sklearn` offers something called a `GridSearchCV` (or `RandomizedSearchCV`) that facilitates trying various combinations of parameters. For brevity, though, we're just going to vary the number of topics and evaluate the two metrics we just examined. Specifically, we'll consider topics between 50 and 150, counting by 10s. We'll collect the metrics and evaluate the results at the end:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for top in range(50,151,10):\n",
    "    print(f\"Fitting {top} topics\")\n",
    "    record = {'topics':top}\n",
    "    lda = LDA(n_components = top,\n",
    "             doc_topic_prior = 25/top,\n",
    "             topic_word_prior = 0.15,\n",
    "             n_jobs = -1,\n",
    "             random_state=123)\n",
    "\n",
    "    lda.fit(dtm) # note I'm just \"fitting\" here, not \"fit_transform\"\n",
    "    record['perplexity'] = lda.perplexity(dtm)\n",
    "    umass = metric_coherence_gensim(measure = \"u_mass\",\n",
    "                        top_n = 5,\n",
    "                        topic_word_distrib = lda.components_,\n",
    "                        dtm = dtm.todense(),\n",
    "                        vocab = vocab,\n",
    "                        texts = None) \n",
    "    record['mean_umass'] = np.mean(umass)\n",
    "    records.append(record)\n",
    "\n",
    "diagnostics = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig,ax = plt.subplots()\n",
    "ax2 = ax.twinx()\n",
    "ax.plot(diagnostics['topics'],diagnostics['perplexity'])\n",
    "ax2.plot(diagnostics['topics'],diagnostics['mean_umass'],color='red',linestyle='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot seems to suggest that there is a lot of variation in both of these scores, but note the scale--this is very zoomed in! It actally seems like things are relatively flat.\n",
    "\n",
    "In addition, perplexity in scikit-learn's implementation has a known issue--it should not be increasing with the number of topics. In my experience, it seems to be inverted--you can still use the elbow method, but the plot is flipped. However, most references suggest using other means to pick the best model, like the coherence measure we computed, along with your own intuition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Task "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We focused completely on LDA in this demo, but we also covered NMF. On your own, I'd like you to try to fit an NMF model. You can use the same settings we used in our first LDA model (100 topics, simple word counts). You should be able to use the same function we wrote above to examine the topics produced by NMF. The documentation for NMF can be found __[here](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html)__. \n",
    "\n",
    "**HOMEWORK** (for disucssion): What are the 5 words associated with the most prevalent topic under this method using the 10% threshold we used previously (**NOTE**: Unlike LDA, NMF does not produce topic probabilities by document)? To transform to something that looks like LDA probabilities, or numbers summing to 1 by row, run `(res / res.sum(axis=1)[:,None])` on your document-topic matrix (assumed to be `res` in that code). Discuss your answer to this question, and the perceived quality of topics on the discussion board (no need to run coherence scores, etc.). \n",
    "\n",
    "I'll help you get started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "nmf = NMF(n_components=100, random_state=123) # while NMF can be deterministic, sklearn has several different intialization options, some of which have a random element."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
