{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d21327a-1098-4bfd-b61c-0ed869c8a8e9",
   "metadata": {},
   "source": [
    "# Demo 7B\n",
    "The purpose of this demonstration is to reiterate learning objects in lectures 7.2-7.3. After completing this demonstration, you should feel comfortable:\n",
    "\n",
    "- using transfer learning to finetune a pretrained deep learning model\n",
    "- evaluate model performance\n",
    "- apply a pretrained model to new data\n",
    "\n",
    "### Transfer Learning in Python\n",
    "Recall that transfer learning refers to the process of updating a deep learning model for a specific task. You can use transfer learning for NLP tasks including:\n",
    "\n",
    "- text classification (sentiment, topics)\n",
    "- masked language models (learning new embeddings)\n",
    "- token classification\n",
    "- question answering (text generation)\n",
    "\n",
    "My experience with transfer learning rests almost exclusively in the first on this list, text classification. This is what we will focus on.\n",
    "\n",
    "In Python, the de facto standard for sharing trained deep learning models is through a library called __[transformers](https://huggingface.co/docs/transformers/index)__. This library is maintained by a group called HuggingFace (ðŸ¤—). If you navigate to the __[models](https://huggingface.co/models)__ section of the page, you'll see there are over 200,000 \"models\" (essentially sets of ANN parameters) available for download and use (directly or after fine-tuning). For instance, the __[bert-base-uncased](https://huggingface.co/bert-base-uncased)__ is the original uncased BERT model trained for masked language. This model is comprised of 110M parameters.\n",
    "\n",
    "You can also filter the models to focus only on __[those meant for text classification](https://huggingface.co/models?pipeline_tag=text-classification&sort=downloads)__ to see the various tasks and corpora that have been used. For instance, there are models focused on __[tweet classification](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest)__ and __[various financial tasks](https://huggingface.co/models?other=financial-text-analysis)__. This latter model is discussed in __[Huang, Wang, and Yang 2023](https://onlinelibrary.wiley.com/doi/full/10.1111/1911-3846.12832)__.\n",
    "\n",
    "Before we move to our exercise, I want to highlight an important consideration that differs from the approach we've taken in earlier exercises and an additional caveat. First, transformer models are generally designed to work on *shorter spans of text*, such as sentences or paragraphs. For instance, __[this general purpose classifier](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)__ has a max sequence length (`max_seq_length`) of 128. Therefore, for fine-tuning we usually need labeled data at this more granular level, and then we must aggregate back up to the document level if that is the level we require. For shorter texts, like product reviews, customer complaints, or social media posts, this generally doesn't cause an issue.\n",
    "\n",
    "Second, the caveat is that this coding is going to feel a little \"heavier\" than earlier tasks. I purposely did not include this topic in an experiential task because we don't really have sufficient time to learn all of the ins and outs of `transformers` and `pytorch`. So, if you feel a bit overwhelmed, know that I'm much more interested in you observing the process than being able to come up with this code independently. Hopefully with this basic understanding you can identify how to leverage these models in your own work as needed.\n",
    "\n",
    "Let's get started with our exercise!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817a5802-4f4c-42ed-9665-313d387c69e3",
   "metadata": {},
   "source": [
    "### The data\n",
    "To demonstrate the power of transfer learning, we require a dataset of hand-labeled data that we can use to update the ANN weights in the transformer model. There are lots of datasets available, but for our demonstration I'm going to use a set of sentences my coauthors and I hand-labeled for a current working paper, available __[here](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3918531)__. In this paper, we examine how the quality of contributions of Seeking Alpha users vary depending on the company's CSR reputation.\n",
    "\n",
    "In some more recent tests that we've added, we use fine-tuned classifiers to identify language in articles related to fundamental performance. This is the task we'll examine.\n",
    "\n",
    "Let's load the data and inspect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f74a861-221f-448f-b77a-1c4da284a9bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   label   2000 non-null   float64\n",
      " 1   sent    5000 non-null   object \n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 78.3+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/storage/ice-shared/mgt8833/classdata/financial_labeled_data.csv\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6f0caf-9b8d-496b-b411-237e783afd85",
   "metadata": {},
   "source": [
    "So this dataset has **5,000** rows/observations, of which **2,000** are labeled. In the data, the label equals `1` for sentences coded as financial. \n",
    "\n",
    "**PAUSE** and determine what percentage of sentences are classified as relating to financial performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffa8fce6-14f8-4915-935b-354e4ce9802e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Your work goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1555be0-3217-4c57-a2d5-4309698d71ad",
   "metadata": {},
   "source": [
    "Note that it's easiest to use the default labels for our datasets, so we're going to rename \"sent\" to be \"text\" and \"label\" to be \"labels\" and then move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c95f0ee-c5c1-4c27-8134-e02cf9e978b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.rename(columns={\"sent\":\"sentence\",\"label\":\"labels\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ece71ad-027f-4cea-ac94-cda53c72a772",
   "metadata": {},
   "source": [
    "### Fine-tuning with `transformers`\n",
    "To fine-tune our model, we're going to follow a process very similar to this __[tutorial](https://huggingface.co/docs/transformers/training)__ available from Huggingface. We'll proceed in four steps:\n",
    "\n",
    "1. Loading data and setting up datasets for training\n",
    "2. Training (or fine-tuning) the model\n",
    "3. Evaluating model performance\n",
    "4. Applying to new data\n",
    "\n",
    "Let's get started with setting up the data.\n",
    "\n",
    "#### Step 1 - Data Set-up\n",
    "##### Formatting the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b390bd1e-ef91-4111-aac1-9fdad5d5b30b",
   "metadata": {},
   "source": [
    "Unfortunately, `transformers` does not work with native dataframes, which is what we're used to using. There are two reasons for this:\n",
    "\n",
    "- First, data must be represented as __[`tensors`](https://en.wikipedia.org/wiki/Tensor)__. Tensors are a broad class of mathematical representations that facilitate operations, such as gradient calculations.\n",
    "- Second, often the volume of data used by these models is massive, so `transformers` has objects available to set up dataset \"loaders\", or processes to avoid putting all data in memory at once. \n",
    "\n",
    "We don't really need to worry about the second issue for this volume of data, and `transformers` includes some methods for converting pandas data. See the documentation __[here](https://huggingface.co/docs/datasets/tabular_load)__. Note that we could have loaded directly from the CSV, but I started with a dataframe because I'm more comfortable manipulating data with `pandas`.\n",
    "\n",
    "Let's start by isolating the 2,000 rows with labels. **PAUSE** and see if you can do that, saving the result in a dataset called `labeled`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de685dfa-57e0-42c2-b62b-eae50f585ed9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Your work goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a58fdf-4282-449c-bc06-b6b5d996371a",
   "metadata": {},
   "source": [
    "Next, we're going to recast the label to an integer. We will then create two datasets, a training and validation dataset. We'll use `train_test_split` to split the dataframe and then use `from_pandas` to convert the data. Finally, we'll store the datasets in a `DatasetDict`, which is just a convenient wrapper used for datasets formatted for `transformers`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43024254-f2aa-4870-bf45-87d0b5a7afa8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labeled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DatasetDict\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Convert label to integer\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m labeled[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mlabeled\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Train/Test split\u001b[39;00m\n\u001b[1;32m      8\u001b[0m train,test \u001b[38;5;241m=\u001b[39m train_test_split(labeled,train_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.80\u001b[39m,random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m123\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'labeled' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Convert label to integer\n",
    "labeled['labels'] = labeled['labels'].astype(int)\n",
    "\n",
    "# Train/Test split\n",
    "train,test = train_test_split(labeled,train_size=0.80,random_state=123)\n",
    "train_ds = Dataset.from_pandas(train,split='train',preserve_index=False)\n",
    "test_ds  = Dataset.from_pandas(test,split='test',preserve_index=False)\n",
    "\n",
    "ds = DatasetDict({\n",
    "    'train': train_ds,\n",
    "    'test':test_ds})\n",
    "ds['train'][100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a6afb8-6e42-42fd-b3ba-4498b55895cd",
   "metadata": {},
   "source": [
    "##### Tokenizing the data\n",
    "Similar to other NLP tasks, we're next going to tokenize the data. However, we can't use our standard `CountVectorizer` approach here. There are two primary reasons for this. \n",
    "\n",
    "First, since we are fine-tuning an existing model, our vocabulary is pre-established. That is, the original model was built with a specific vocabulary, and we must start with that. There are methods for adding new language to the vocabulary, but usually this isn't necessary. One exception would be adding emojis for social-media classification, but even that we can circumvent by starting with a model built for social media.\n",
    "\n",
    "Second, recall that much of the power of transformers comes from the concept of attention, which requires information on sequence. Thus, our tokenizer needs to capture this information.\n",
    "\n",
    "Fortunately, Huggingface has made the tokenization process very easy by \"attaching\" the appropriate tokenizer to each model we'll use. There are many tokenization objects depending on the type of model you are fine-tuning, but `AutoTokenizer` available through Huggingface's __[Auto Classes](https://huggingface.co/docs/transformers/model_doc/auto)__ is a great choice for general tokenization.\n",
    "\n",
    "To set up the tokenizer, we need to import a few objects and identify the model we're going to fine-tune. We're going to use a version of \"DistilBERT\", which is a simplified version of BERT (67M parameters instead of well over 100M). The specific version we'll use is called __[`distilbert-base-uncased-finetuned-sst-2-english`](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878b2041-4e6d-43b8-88cf-b1654c26473e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "pretrained_model = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model,padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b9cfc2-f58a-4afc-b5f8-d9ad702e8a40",
   "metadata": {},
   "source": [
    "Next, we need to use this `tokenizer` to format our data. There are a few ways of doing this; we'll follow Huggingface's recommended approach in the tutorial I link to above. Specifically:\n",
    "\n",
    "1. Write a function that accepts each record and returns tokenized data.\n",
    "2. Use __[`map`](https://huggingface.co/docs/datasets/process)__ to apply that function to our formatted datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84de93c3-1ea7-4e10-92b5-ffa09c203365",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(record):\n",
    "    return tokenizer(record['sentence'], padding='max_length', truncation=True)\n",
    "\n",
    "tokenized_data = ds.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a9ec3f-a86e-4c37-8d0a-a784bb2a36c0",
   "metadata": {},
   "source": [
    "Let's recap a few things here:\n",
    "- `padding`: We set this to \"max_length\", which is based on the pre-trained model. We could also make this corpus-specific (longest sequence), or don't pad sequences at all (the default). I've always used fixed-length sequences in this context.\n",
    "- `truncation`: We set this to `True`, which means any sequences longer than the maximum length will be truncated.\n",
    "- `map`: The `dataset` method mentioned above.\n",
    "- `batches`: Allows you to process the data in batches; we only have 2,000 records, so this probably isn't necessary, but this illustrates how you'd want to handle larger datasets.\n",
    "\n",
    "One additional thing to note. The number of tokens generated by these tokenizers doesn't always equal the number of words (!!). This is for a two primary reasons. First, these models insert special tokens, `[CLS]` and `[SEP]`. We won't see those tokens, but they are encoded to denote the beginning and end of sentences. Second, some tokenizers actually tokenize \"sub-words\", like separating \"disagree\" into \"dis\" and \"##agree\", to help better understand meaning.\n",
    "\n",
    "If you want to see what one element of tokenized data looks like, uncomment this line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c891ea28-d8bd-437c-9a69-02737c3f4f2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenized_data['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464fd1d4-2812-459f-a710-feb6efe67e81",
   "metadata": {},
   "source": [
    "Now we're ready to train!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f6fa89-b54d-4a0c-a3f1-03dfa9ecacda",
   "metadata": {},
   "source": [
    "#### Step 2 - Training\n",
    "If you read through the \n",
    "`transformers` documentation, you'll see there are numerous options, including a simple __[`Trainer`](https://huggingface.co/docs/transformers/v4.30.0/en/main_classes/trainer#transformers.Trainer)__ option, training with native pytorch, or even with tensorflow if your original model was build in that architecture. In my experience, I haven't had to go beyond the most direct method of fine-tuning, the `Trainer` object, so that's what we will focus on.\n",
    "\n",
    "Let's get our model set up. Note that the model must match the tokenizer, so we'll use the same `pretrained_model` variable we established above (and you are free to try other models by changing that variable). This may take a minute if you download the model from Huggingface. I'm also going to set several random seeds to try to make this as reproducible as possible, though it probably won't be perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567b78d6-7a16-42da-beff-f1ad997132e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch, numpy as np, random\n",
    "# Set random seeds\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val) # don't really need this unless using GPU\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(pretrained_model, num_labels=2,\n",
    "                                                          ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1514da33-56c5-4d54-a24d-0857dde63108",
   "metadata": {},
   "source": [
    "We now have to set up our training arguments. We do this with a special object called `TrainerArguments` (__[docs](https://huggingface.co/docs/transformers/training#:~:text=Next%2C%20create%20a-,TrainingArguments,-class%20which%20contains)__). We will leave __[hyperparameters](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments)__ mostly as the defaults but adjust a few things:\n",
    "- `output_dir` = \"./trained_model\"\n",
    "- `learning_rate` = `1e-5` (matching to original model)\n",
    "- `warmup_steps` = 5 (how many iterations before evaluation begins)\n",
    "- `num_train_epochs` = 5 (how iterations through the data)\n",
    "- `evaluation_strategy` = \"steps\" (when to evaluate, after certain steps, set in `eval_steps`, or after epochs)\n",
    "- `per_device_train/eval_batch_size` = 16 / 64(batch sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04178e68-8600-4030-9087-c5119012df05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import os\n",
    "\n",
    "outdir = \"./trained_model\"\n",
    "logdir = \"./logs\"\n",
    "for d in [outdir,logdir]:\n",
    "    os.makedirs(d,exist_ok=True)\n",
    "\n",
    "params = TrainingArguments(output_dir = outdir,\n",
    "                           overwrite_output_dir = 'True',\n",
    "                           learning_rate=1e-5,\n",
    "                           weight_decay=0.001,\n",
    "                           warmup_steps = 5,\n",
    "                           num_train_epochs=5,\n",
    "                           evaluation_strategy=\"steps\",\n",
    "                           logging_dir=logdir,\n",
    "                           per_device_train_batch_size=16,\n",
    "                           per_device_eval_batch_size=64,\n",
    "                           save_strategy='steps',\n",
    "                           eval_steps=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691cfe75-20bf-4a99-a4cf-ca3cb9f99956",
   "metadata": {},
   "source": [
    "As noted in the Huggingface documentation, the `Trainer` does not automatically evaluate model performance. There's a convenient library called __[evaluate](https://huggingface.co/docs/evaluate/index)__ that allows us to access various functions we can \"plug-in\" for evaluation as the model trains. However, we can also use the `sklearn` functions we are used to. We just have to set them up in a function.\n",
    "\n",
    "This function is going to accept `pred` which is a set of predictions from the transformer (we'll see this again later). Note that transformers models always return \"logits\", which require conversion for interpretation. We'll use `argmax` to identify the predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f0e7ef-d304-48b2-92aa-4928331baad2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4babccae-3b7b-4f6a-a5ce-312d25aa2226",
   "metadata": {},
   "source": [
    "We're now going to set up an early stopping callback. This is far more complicated than in `keras` because the `Trainer` method doesn't have a built in callback parameter. We haven't ever defined a custom `class`, so I'm not going to go through the details here. Just recognize that this object is setting up a callback function to evaluate accuracy on the fly, and interupt training if no further improvement is detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672861f2-311d-49f0-beee-7c62638a0194",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback, TrainerControl\n",
    "\n",
    "class EarlyStoppingCallback(TrainerCallback):\n",
    "    \"A callback that implements early stopping.\"\n",
    "    def __init__(self, early_stopping_patience=1):\n",
    "        self.early_stopping_patience = early_stopping_patience\n",
    "        self.early_stopping_counter = 0\n",
    "        self.best_metric = None\n",
    "        self.last_metric = None\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        # We assume that 'eval_accuracy' is logged by the Trainer\n",
    "        if 'eval_accuracy' in logs:\n",
    "            self.last_metric = logs['eval_accuracy']\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "        # metric for early stopping\n",
    "        metric = self.last_metric\n",
    "        if self.best_metric is None:\n",
    "            self.best_metric = metric\n",
    "        # if current metric is worse than best_metric, increment counter\n",
    "        if metric < self.best_metric:\n",
    "            self.early_stopping_counter += 1\n",
    "        else:  # else, reset counter and update best_metric\n",
    "            self.early_stopping_counter = 0\n",
    "            self.best_metric = metric\n",
    "        # if counter has reached the patience limit, stop training\n",
    "        if self.early_stopping_counter >= self.early_stopping_patience:\n",
    "            control.should_training_stop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8e3704-cda8-468f-8c97-949a44f00d4e",
   "metadata": {},
   "source": [
    "Next, we set up our `Trainer` with the objects established above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91da9383-cb42-4ba5-994b-fe01e1974f30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                  args=params,\n",
    "                  train_dataset=tokenized_data['train'],\n",
    "                  eval_dataset=tokenized_data['test'],\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  callbacks=[EarlyStoppingCallback(early_stopping_patience=3)] # set patience at 3\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a3ffb6-7e25-447a-9bd3-6f2b1f3b24ef",
   "metadata": {},
   "source": [
    "Finally, we can train the model with the `train()` method. This will take quite a while unless you utilize a GPU, which I have not done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4806b24-9349-4ae5-a515-c73d8115aaf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cf66a7-9685-499c-924d-dbde169f3c31",
   "metadata": {},
   "source": [
    "This took quite a while, and ended up around 80% when I finished. That's not bad, but not great. We did a little better in the paper, though I think I started from a different model. In addition, this data is noisy. If you inspect several sentences you'll see they aren't very well formed in some cases. This will add noise. \n",
    "\n",
    "Also, I ran this on my laptop's CPU. With GPUs, training time is *drastically* reduced. You could set up a GPU instance on ICE though your environment would need to be adjusted. Google's Colab is also a great resource for free GPUs for smaller tasks. \n",
    "\n",
    "We're going to save the model, but before we get there I want to make one adjustment. Since we are fine-tuning a pre-existing model, the original labels, \"POSITIVE\" and \"NEGATIVE\" are still in the model configuration. It won't be apparent unless we generate new predictions from the model, but let's go ahead and update this configuration before saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba402488-49b6-4fd1-85d9-d0afb9f46ecf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.config.id2label = {0: \"NON-FINANCIAL\", 1: \"FINANCIAL\"}\n",
    "model.config.label2id = {\"NON-FINANCIAL\": 0, \"FINANCIAL\": 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe2114e-8b47-4c0a-9a34-10883cbfb5c4",
   "metadata": {},
   "source": [
    "Now we will save the model before going any further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa517dba-3f4d-4d56-a418-699244378f90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.save_model('final_fin_classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f590f6f9-bc6c-406d-81f4-ffd101475f52",
   "metadata": {},
   "source": [
    "#### Step 3 - Evaluating Model Performance\n",
    "Next we're going to evaluate this model as we would other classifiers we've trained. Since I'm guessing many of you won't go through the full training procedure, let's load the saved model I've provided you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efae6f10-0eaf-42fd-a308-1bc35046bf9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model2 = AutoModelForSequenceClassification.from_pretrained(\"/storage/ice-shared/mgt8833/classdata/final_fin_classifier\", num_labels=2,\n",
    "                                                          ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcd1319-9471-4200-8931-f8d22da2b65f",
   "metadata": {},
   "source": [
    "To use this model to generate preditions, we have two options. \n",
    "\n",
    "First, we can take our training (or testing) dataset which we've already encoded and use `torch` to generate predictions. This is actually fairly involved because `torch` is a lower level framework than what we're used to (`sklearn`, `transformers`, etc.). I also have had issues keeping things aligned when generating batches in `torch`, so I usually avoid this approach.\n",
    "\n",
    "Second, we can build a __[pipeline](https://huggingface.co/docs/transformers/main_classes/pipelines)__ that accepts our original (textual) data, tokenizes, and then classifies on the fly. The beauty of this approach is we can use it with any text (as we'll see below).\n",
    "\n",
    "So we'll go with this latter approach, and here are the specific steps we'll take:\n",
    "1. Load both the **model** and **tokenizer** (we have already done this; we'll use `model2` and `tokenizer`).\n",
    "2. Define a `pipeline`. \n",
    "3. **Generate predictions** for your data and **evaluate results**.\n",
    "\n",
    "Let's start with **step 2** since we've already loaded our model and tokenizer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0c1593-b1cc-4477-8ad0-7b33d9f4ae0f",
   "metadata": {},
   "source": [
    "**Step 3.2 - Build the pipeline**:\n",
    "\n",
    "A __[pipeline](https://huggingface.co/docs/transformers/main_classes/pipelines)__ provides a very flexible and intuitive approach to setting up a function-like object to process new text. The first (required) positional argument identifies the *type* of pipeline (\"text-classification\" for us, but see documentation for other options). After that, we provide the model and tokenizer. Different types of pipelines will require different objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1f9fcc-d8e3-469c-959d-e8190d4b6893",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create the pipeline\n",
    "text_classification_pipeline = pipeline(\"text-classification\", model=model2, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07e711f-3ab9-4751-ab10-4621f4e9bdb8",
   "metadata": {},
   "source": [
    "**Step 3.3 - Generate predictions and evaluate results**\n",
    "\n",
    "Now, we can pass our original text to this pipeline and generate predictions. Let's focus on the evaluation data, which is in the dataframe `test`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf1e3f3-a208-4a51-95c1-b6754c3b23b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0c171a-15ea-4ba8-a9ff-03d43ee750df",
   "metadata": {},
   "source": [
    "We can pass this list of sentences to our pipeline like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ebd632-a564-4d21-938d-3999e656a552",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = text_classification_pipeline(test['sentence'].tolist())\n",
    "predictions[:10] # show first 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571cda76-82f6-40c6-8a50-d9bca056f535",
   "metadata": {},
   "source": [
    "Now we'll create a dataframe from this list, and convert our labels to the original \"ids\" (0 or 1) with the `label2id` dictionary stored in the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba5b8b1-02ff-4aa6-be11-bbe7f0f88a5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_pred_df = pd.DataFrame(predictions)\n",
    "test_pred_df['label'] = test_pred_df['label'].map(model2.config.label2id)\n",
    "test_pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0108f6-b38d-4ba6-a9e5-21804014ced4",
   "metadata": {},
   "source": [
    "Finally, we can use `classification_report` to get more insight into how this model performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84376537-dd67-458d-9957-86d144a65a2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(test['labels'],test_pred_df['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbe2c99-d9b1-4c31-b498-4ff145ac9d98",
   "metadata": {},
   "source": [
    "#### Step 4: Applying to new data\n",
    "Often, we use a transformer model like the one fine-tuned in this demonstration as part of a larger \"pipeline\" (a larger pipeline that includes the pipeline we defined above). In other words, we have some analytical procedure where we process large volumes of text, and part of that procedure relates to *coding* or *classifying* elements in the corpus according to some label. Perhaps your organization receives large volumes of customer feedback (i.e., complaints). You could use a fine-tuned transformer to efficiently direct complaints to the appropriate party of prioritize ones that are particularly impactful.\n",
    "\n",
    "To finish up this demonstration, we're going to look at how our model performs on *new text*. Recall at the beginning of this demo we created a dataframe called `labeled`, which excluded any sentences in the original dataframe (`df`) without labels. Let's go back to the original dataframe and grab a sample of unlabeled sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d9c4c7-a633-42b5-b256-34e512cee28c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_sents_df = df.loc[df['labels'].isnull(),'sentence'].sample(5,random_state=321)\n",
    "new_sents_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590b460f-6d5d-4d8f-b583-ebc906b85096",
   "metadata": {},
   "source": [
    "Now, we'll convert this to a list and generate our predictions, just like we did above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e9bddd-6eee-4d46-9b26-192955d7b884",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "predictions = text_classification_pipeline(new_sents_df.to_list())\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9462f6fa-6085-49ee-9e57-b10c99608fb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the labels and scores\n",
    "for sentence, prediction in zip(new_sents_df, predictions):\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Predicted label: {prediction['label']}\")\n",
    "    print(f\"Confidence score: {prediction['score']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5434461-6cbc-4a67-a4fa-e501047441f4",
   "metadata": {},
   "source": [
    "Not bad! These labels seem reasonable to me (and again, remember, this data was unlabeled)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd24899-8400-4dad-b347-1c98d4ff68b6",
   "metadata": {},
   "source": [
    "#### Conclusion & Homework\n",
    "Hopefully this demo provided you a reasonable handle on the power of transfer learning, particularly as it relates to text classification. Transformer-based models have revolutionalized NLP in a variety of settings. While we focused on the base implementation of transformers (with `transformers`), there is a \"wrapper\" package available called __[simpletransformers](https://simpletransformers.ai/)__. I did not use this in our demo because (1) it's relatively new, (2) has been inconsistently maintained (though it seems better recently), and (3) hides some of the details I wouldn't to make explicit. In any case, I wanted to point it out in case you want to explore more on your own.\n",
    "\n",
    "For your **homework**, I'd like you to identify a situation in your own professional experience where applying transfer learning could help add insights or make analyses more efficient. Discuss with your classmates on the discussion board."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:week7b]",
   "language": "python",
   "name": "conda-env-week7b-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
