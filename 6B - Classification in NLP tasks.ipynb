{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 6B\n",
    "The purpose of this demonstration is to reiterate learning objects in lectures 6.2 and 6.4-6.5. After completing this demonstration, you should feel comfortable:\n",
    "- loading and preparing textual data for analysis with classification-type supervised methods\n",
    "- training a variety of classifiers and evaluating performance with accuracy, precision, and recall\n",
    "- fine-tuning classifiers with randomized parameter searches\n",
    "- evaluating the performance of ensemble models, including a Random Forest and LightGBM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data, Preprocessing, and Feature Extraction\n",
    "For this demonstration, we are going to use the same data, preprocessing, and tokenization procedures as in the prior demonstration. The main difference this time, though, is that we are going to focus on *sentiment* instead of *sentiment_score*. Let's look at this column and define a variable for our classification task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and fix formatting\n",
    "import pandas as pd, numpy as np, ftfy\n",
    "data_path = \"/storage/ice-shared/mgt8833/classdata/stocktwits_sample.csv.gz\"\n",
    "df = pd.read_csv(data_path)\n",
    "df = df.replace({np.nan: 'None'}) #ICE loaded these in as \"NaN\"\n",
    "df['text'] = df['text'].apply(lambda x: ftfy.ftfy(x))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, *sentiment* is missing for some observations. Let's look at the count by unique value of *sentiment*. **PAUSE** and generate those counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your own work goes here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use some `loc` commands to create an indicator variable that takes the value 1 for Bullish, 0 for Bearish, and missing for \"None\". Instead, though, let's use a pandas function that's designed to generate \"one hot\" encodings, `pd.get_dummies()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(df['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add this to our dataframe with `pd.concat`, remove observations with the value \"None\" (though keep in another dataframe), and rename the Bullish column to be simply \"Y\", since that's what we're going to use in our classifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.concat([df,pd.get_dummies(df['sentiment'])],axis=1)\n",
    "no_sent = df2.loc[df2['None']==1]\n",
    "df2 = df2.loc[df2['None']==0].rename(columns = {\"{'basic': 'Bullish'}\":\"Y\"})\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now move to the rest of our set up procedures. We will keep everything the same as in Demo 6A:\n",
    "1. Use custom tokenizer that is meant to work with tweets and addresses cash tags. \n",
    "2. Remove stop words and perform other normalization / preprocessing steps.\n",
    "3. Use TF-IDF weighting to extract features. We'll allow for single words and bigrams, but only retain the top 1,000 features since we have less data. I'm also going to limit the matrix to words that appear in no more than 50% of tweets.\n",
    "4. Split the data into a training and validation set.\n",
    "The following cells run through this process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll set up our tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up tokenizer\n",
    "import emoji, re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stops = stopwords.words('english')\n",
    "cashtag_rx = re.compile(r'\\$[a-z0-9.]+?\\b',re.I)\n",
    "\n",
    "twtokenize = TweetTokenizer()\n",
    "\n",
    "def myTweetTokenizer(tweet):\n",
    "    tweet = re.sub(cashtag_rx,'cash_tag',tweet) # get rid of cash tags\n",
    "    toks = twtokenize.tokenize(tweet)\n",
    "    good_tokens = []\n",
    "    for tok in toks:\n",
    "        if emoji.is_emoji(tok):\n",
    "            good_tokens.append(tok)\n",
    "        elif tok=='cash_tag':\n",
    "            good_tokens.append(tok)        \n",
    "        elif len(tok)>=3 and tok.isalpha():\n",
    "            good_tokens.append(tok.lower())\n",
    "        elif tok.startswith(\"#\"):\n",
    "            good_tokens.append(tok.lower()) #we'll keep lowercase hashtags    \n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return good_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll extract our features (and check the top 10 words):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vec = TfidfVectorizer(lowercase = False, tokenizer = myTweetTokenizer, ngram_range = (1,2), max_features = 1000, stop_words = stops, max_df=0.50)\n",
    "\n",
    "dtm = tfidf_vec.fit_transform(df2['text'])\n",
    "\n",
    "vocab = np.asarray(tfidf_vec.get_feature_names_out())\n",
    "\n",
    "topn = 10\n",
    "print(f\"{topn} most common words based on word counts:\\n\")\n",
    "words = vocab[np.asarray(dtm.todense()).sum(axis=0).argsort()[-topn:][::-1]]\n",
    "print(\", \".join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll split the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trainX,validX,trainy,validy = train_test_split(dtm, df2['Y'],train_size=0.80,random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Simple Classifiers with `sklearn`\n",
    "We'll begin by training and evaluating performance of two common classifiers for NLP-related tasks, Decision Trees and the Naive Bayes Classifier (NBC). You'll notice the process for training these models is very similar to the approach we used in our regression tasks, though obviously the hyperparameters we set in each classifier vary.\n",
    "#### Decision Trees ####\n",
    "For the __[Decision Tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)__, we'll consider the following hyper parameters:\n",
    "- `criterion`: how the quality of the \"split\" is measured (or how \"good\" a given leave is); we will leave this as the default (\"gini\") for now.\n",
    "- `max_depth`: how big the tree can get; defaults to `None`, which guarantees an overfit tree. We'll use 200.\n",
    "- `max_features`: how many features the tree can use. We'll use \"sqrt\", which uses the square root of the number of features.\n",
    "- `class_weight`: whether to assign unequal weights to observations. We won't use this yet, but we'll come back to it later.\n",
    "\n",
    "To evaluate the quality of the fit, there are a number of metrics we can use. We're going to use two functions, `confusion_matrix` and `classification_report`. The former is a contingency table, or 2x2 presentation of Type I and Type II errors. The latter provides detailed information on precision, recall, and the overall F1 score.\n",
    "\n",
    "Let's train our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "\n",
    "tree = DTC(max_depth = 200, max_features = 'sqrt', random_state = 123)\n",
    "tree.fit(trainX,trainy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sklearn` metrics commands accept two position arguments, `y_true` and `y_pred`. The \"true\" values, `y_true`, are simply the assigned labels for the data we are classifying. The predicted values, `y_pred`, should be generated from our fitted model.\n",
    "\n",
    "We will now generate the __[`confusion_matrix`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)__ and __[`classification_report`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "cm = confusion_matrix(trainy,tree.predict(trainX))\n",
    "\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix presents the true values as rows, and predicted values as columns. To illustrate, compute the number of true positives (the bottom row) and compare to the number of true positives in `trainy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm[1,:].sum()==trainy.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, accurate models have lots of observations on the diagonal, and few off diagonal. We can easily compute overall accuracy by comparing the number of observations on the diagonal with the total:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(cm[0,0] + cm[1,1])/cm.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WOW!!**\n",
    "\n",
    "Is there a problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(validy,tree.predict(validX))\n",
    "\n",
    "(cm[0,0] + cm[1,1])/cm.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model still performs relatively well, though not nearly as good as on the trained data. But that makes sense--decision trees **will** eventually **perfectly** fit the data if given enough leeway. We limited the number of features and depth, so perfect prediction didn't happen, but it was close.\n",
    "\n",
    "Recall that precision measures the quality of the classification from a \"signal to noise\" perspective. Recall measures the quality of the model based on it's ability to avoid \"false negatives\".  Often recall may be more important than precision in a given model, or vice versa. For instance, fire fighters check on pretty much all automated alarm calls (if not cancelled by customer) even though the majority are false alarms. \n",
    "\n",
    "Let's compute precision and recall for the 1s in this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Precision is {round((cm[1,1]/cm[:,1].sum())*100,2)}%.\") # bottom right divided by total in right column\n",
    "print(f\"Recall is {round((cm[1,1]/cm[1,:].sum())*100,2)}%.\") # bottom right divided by total in bottom row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are precision and recall both much higher than overall accuracy? \n",
    "\n",
    "The model is **biased**, which we can see if we examine precision and recall for the \"0\" class (how often was a *bearish* prediction correct):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Precision is {round((cm[0,0]/cm[:,0].sum())*100,2)}%.\") # bottom right divided by total in right column\n",
    "print(f\"Recall is {round((cm[0,0]/cm[0,:].sum())*100,2)}%.\") # bottom right divided by total in bottom row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix is a great way to examine model performance at a very granular level, but we can easily generate metrics we just went through using `sklearn`'s `classification_report`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(validy,tree.predict(validX)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've talked about precision, recall, and accuracy (a model level metric). Let's run through the other items on this report:\n",
    "- `f1-score`: the geometric mean of precision and recall (i.e., sqrt(precision\\*recall))\n",
    "- `support`: the number of observations contributing to the given row (i.e., the number of 0s or 1s, or total)\n",
    "- `macro avg`: the average value of the given class-level metric\n",
    "- `weighted avg`: the weighted average value of the given class-level metric (i.e., majority class more heavily influences statistic)\n",
    "\n",
    "There are a number of ways we can try to deal with model bias, such as resampling (under or oversampling), reducing complexity, or, for some models, using class weights. Class weights default to equal, but if we \"value\" minority class observations more, then then model will shift parameters towards more accurate classification of those at the expense of the majority class. \n",
    "\n",
    "Class weights are a hyperparamter that can be tuned. Let's see what the model looks like if we try to approximate roughly equal weights. which we can do by setting the `class_weight` hyperparameter equal to \"balanced\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DTC(max_depth = 200, max_features = 'sqrt', random_state = 123,class_weight='balanced')\n",
    "tree.fit(trainX,trainy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(validy,tree.predict(validX)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This didn't help at all. We can try more aggressive class weights using a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cws = {0:10,1:1}\n",
    "tree = DTC(max_depth = 200, max_features = 'sqrt', random_state = 123,class_weight=cws)\n",
    "tree.fit(trainX,trainy)\n",
    "print(classification_report(validy,tree.predict(validX)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This did slightly change performance, but it doesn't seem to be materially better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes Classifier\n",
    "Scikit learn offers several implementations of the Naive Bayes (NB) models. For classification, we'll focus on the __[Gaussian NB model](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB)__. This model is much simpler to implement since it has few hyperparameters. In fact, there are only 2:\n",
    "- `priors`: only updated if we have prior probability expectations that differ from the data (we don't)\n",
    "- `var_smoothing`: a smoothing parameter we'll leave as the default\n",
    "\n",
    "So let's implement!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB as NBC\n",
    "nbc = NBC()\n",
    "nbc.fit(trainX.toarray(),trainy)\n",
    "print(classification_report(validy,nbc.predict(validX.toarray())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model did not work well. Precision for the \"1s\" is pretty good, but otherwise it's not great.\n",
    "\n",
    "We can try to adjust the `var_smoothing` hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbc = NBC(var_smoothing=1)\n",
    "nbc.fit(trainX.toarray(),trainy)\n",
    "print(classification_report(validy,nbc.predict(validX.toarray())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This performs much better now, but like before it's biased. Before moving on, this is an important illustration of the fact that the default hyperparameters are often far from the best choices, which is why model tuning is so important. We'll return to a tuning exercise towards the end of this demo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent (SGD)\n",
    "As discussed in the lectures, SGD is not, itself, a classification technique. Rather, it's an optimization technique that can improve performance of more traditional classification methods. Ones that historically aren't well suited for text can be adapted for an NLP-related task by increasing the efficiency of optimization.\n",
    "\n",
    "Let's explore how SGD performs using a few different implementations of `sklearn`'s __[SGDClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html)__. This classifier implements a variety of \"regularized linear models\" for classification tasks. Some key parameters include the following:\n",
    "- `loss`: This defines the loss function to be used, which really controls what type of model you're implementing. For instance `hinge` (the default) uses a support vector machine, `log_loss` implements a logistic regression, and `modified_huber` a third implementation designed for classification tasks. There are several others as well, which we'll look at below.\n",
    "- `penalty`: This allows you to select the type of regularization penalty to include in the loss function. It defaults to \"L2\" (which is based on sum of squared feature importances). We'll use \"L1\" to encourage sparsity of feature selection.\n",
    "\n",
    "There are many other hyperparameters related to how the model is trained, maximum iterations, tolerances, etc. that we will leave as defaults for now. Let's set up a loop that tries the three primary classification-based loss functions and outputs the model fit statistics. We'll save each model as we go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier as SGD\n",
    "fitted_models = {}\n",
    "loss_functions = ['hinge','log_loss','modified_huber', 'squared_hinge', 'perceptron']\n",
    "for loss in loss_functions:\n",
    "    sgd = SGD(loss=loss, penalty='l1', n_jobs=-1,random_state=123)\n",
    "    print(f\"Running SGD model with {loss} loss function.\")\n",
    "    sgd.fit(trainX.toarray(),trainy)\n",
    "    print(f\"Classification Report for this model:\\n\\n\")\n",
    "    print(classification_report(validy,sgd.predict(validX.toarray())))\n",
    "    fitted_models[loss] = sgd\n",
    "    print(\"------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we see that performance is good to very good for classifying bulls, but the bears are much harder to classify. On the one hand, this is to be expected since there's much less data available for those observations (hence the name \"minority class\"). On the other hand, some of this could be due to overfitting and balanced class weights. We'll do some more robust tuning later.\n",
    "\n",
    "Since these are inherently linear models, so we can explore the features that are most important in each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topn = 10\n",
    "for loss in loss_functions:\n",
    "    coefs = fitted_models[loss].coef_\n",
    "    print(f\"For the {loss} SGD model, the following {topn} features were most important:\\n\")\n",
    "    for c in np.abs(coefs).argsort()[0][::-1][:topn]: # computes absolute values, and argsorts, flips to descending, and grabs first \"n\"\n",
    "        print(f\"\\t{vocab[c]} has a coefficient of {coefs[0][c]}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, suppose you wanted to see the 5 most positive, and 5 most negative feature importances. How could you adapt the procedure above to print two separate sets of coefficients? **PAUSE** and try to answer that question on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your own work goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Models\n",
    "Recall that an ensemble model refers to the class of approaches for using estimates from multiple models to form one final prediction. `sklearn` provides classes that you can use to build customized ensembles:\n",
    "- __[`BaggingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)__: Facilitates setting up a \"bag\" of weak learners\n",
    "- __[`AdaBoostClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)__: Facilitates setting up recursive set of \"boosted\" learners that learn from errors in previous model.\n",
    "- __[`StackingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html#sklearn.ensemble.StackingClassifier)__: Facilitates setting up stacked model that combines output of first level learners in final predictive model.\n",
    "\n",
    "These objects are great for customizing your own ensemble model. We're going to explore two \"prepackaged\" ensemble models, the Random Forest Classifier and LightGBM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "`sklearn`'s `RandomForecastClassifier` (__[documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)__) facilitates modeling a random forest, or a collection of \"weak\" learner decision trees. It has several hyperparameters that require tuning. Many (if not most) of them overlap with the decision tree classifier (e.g., `criterion`, `max_depth`, `max_features`, etc.). \n",
    "\n",
    "The main \"new\" parameters that I focus on are:\n",
    "- `n_estimators`: the size of the forest, or number of trees. Defaults to 100.\n",
    "- `max_samples`: The number of observations to use to train each tree (requires `bootstrap` be `True`, which is the default). The default is None (so all samples), but we can set it to something smaller which can help with overfitting.\n",
    "\n",
    "Let's train a few forests, varying only `n_estimators` and `max_samples` so we can explore how performance and bias changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "n_estimators = [50,100,250]\n",
    "max_samples = [1,0.75,0.25]\n",
    "\n",
    "for n in n_estimators:\n",
    "    for m in max_samples:\n",
    "        rf = RFC(n_estimators=n, max_samples=m, n_jobs = -1, random_state=123, class_weight='balanced')\n",
    "        print(f\"Running Random Forest model with {n} estimators and a max of {round(m*100,2)}% observations per tree.\")\n",
    "        rf.fit(trainX.toarray(),trainy)\n",
    "        print(f\"Classification Report for this model using validation data:\\n\\n\")\n",
    "        print(classification_report(validy,rf.predict(validX.toarray())))\n",
    "        print(f\"Classification Report for this model using training data:\\n\\n\")\n",
    "        print(classification_report(trainy,rf.predict(trainX.toarray())))\n",
    "        print(\"------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a lot of output to sift through, but this illustrates two things:\n",
    "1. There's significant bias in almost all models. In fact, many predict **0** bearish tweets (hence the error messages).\n",
    "2. Many of the models are severely overfitted. \n",
    "\n",
    "#### Light GBM \n",
    "The final ensemble we'll train is a the \"Light Gradient Boosted Machine\" classification model. I chose to include this one because it frequently performs best in NLP tasks. The Python __[implementation of Light GBM](https://lightgbm.readthedocs.io/en/v3.3.5/index.html)__ is not part of `sklearn`, but it has an `sklearn`-like API available through `LGBMClassifier` (__[docs](https://lightgbm.readthedocs.io/en/v3.3.5/pythonapi/lightgbm.LGBMClassifier.html)__). \n",
    "\n",
    "If you examine the documentation, you'll see that this model has *many* hyperparameters. For brevity, we'll just implement a model with default parameters except one. `reg_alpha` is the L1 regularization term, which defaults to 0. To encourage sparsity I'm going to make that weight 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier as LGBM\n",
    "\n",
    "lgbm = LGBM(reg_alpha=0.01,random_state=123,n_jobs=-1)\n",
    "lgbm.fit(trainX,trainy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(validy,lgbm.predict(validX.toarray())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Becoming familiar... good overall accuracy, but very biased!\n",
    "\n",
    "Now that we've trained basic implementations of a wide range of models, let's conclude with a comprehensive tuning process that tries to identify the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model Selection\n",
    "We've considered X classifiers:\n",
    "1. Decision Tree\n",
    "2. Naive Bayes\n",
    "3. Stochastic Gradient Descent (with several different loss functions)\n",
    "4. Random Forest\n",
    "5. Light GBM\n",
    "\n",
    "We're going to set up a tuning process which re-examines most of these models, but does the following:\n",
    "1. Considers a wide range of hyperparameter values\n",
    "2. Uses five-fold cross-validation\n",
    "3. Optimizes the model on the \"macro\" f1-score\n",
    "4. Saves the \"best\" model\n",
    "\n",
    "Note that we're going to skip the Naive Bayes classifier since it's a little different in that it has a closed form solution and really only has one parameter to tune.\n",
    "\n",
    "#### Set up hyperparameter options for each model ###\n",
    "To start, let's set up our four sets of hyperparameters. Many of these models allow for varying class weights, so we'll start by establishing a set of those to try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "maj_weight = np.random.uniform(0,0.5,100) # 100 random numbers between 0 and 0.5\n",
    "class_weights = [{0: 1-k, 1:k} for k in maj_weight]\n",
    "class_weights.append('balanced') # adds \"balanced\" as an option\n",
    "class_weights.append(None) # adds no weighting as an option\n",
    "class_weights[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll add some model-specific paramters. I've selected up to 5 from each of the model types (usually first 4-5), in addition to class_weights (if available) and a random_state. This is not necessarily comprehensive, but it should allow us to observe a wide range of model configurations. \n",
    "\n",
    "We will set up a dictionary for each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_params = {'criterion':['gini','entropy','log_loss'],\n",
    "             'max_depth':[None,10, 20, 50, 100],\n",
    "             'min_samples_split':[2,5,10], # required to split\n",
    "             'max_features':[None,'sqrt','log2'],\n",
    "             'class_weight':class_weights,\n",
    "             'random_state':[123]\n",
    "            }\n",
    "\n",
    "sgd_params = {'loss':['hinge','log_loss','modified_huber', 'squared_hinge', 'perceptron'],\n",
    "              'penalty':['l1','l2','elasticnet',None], # elasticinet is another regularization technique that encourages sparsity\n",
    "              'alpha':[0.0001, 0.001, 0.01, 0.05, 0.1, 0.5, 1, 5, 10],\n",
    "              'l1_ratio':np.random.uniform(0,1,10).tolist(),\n",
    "              'random_state':[123],\n",
    "              'class_weight':class_weights\n",
    "             }     \n",
    "\n",
    "rf_params = {'n_estimators':[10,25,50,100,200],\n",
    "             'criterion':['gini','entropy','log_loss'],\n",
    "             'max_depth':[None,10, 20, 50, 100],\n",
    "             'min_samples_split':[2,5,10], # required to split\n",
    "             'max_features':[None,'sqrt','log2'],\n",
    "             'max_samples':[None,0.10,0.50,0.75],\n",
    "             'class_weight':class_weights,\n",
    "             'random_state':[123]\n",
    "            }\n",
    "\n",
    "lgbm_params = {'boosting_type':['gbdt', 'dart', 'goss'],\n",
    "               'num_leaves':[10,20,30,40,50],\n",
    "               'max_depth':[-1, 10, 20, 50, 100],\n",
    "               'learning_rate':[0.01,0.1,0.2,0.5], # note this is a very important parameter, as it controls how much \"feedback\" the model has. You can also define a function to make it adaptive\n",
    "               'n_estimators':[25,50,75,100,200],\n",
    "               'class_weight':class_weights,\n",
    "               'random_state':[123]\n",
    "              }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning with RandomizedSearchCV\n",
    "In the previous demonstration, we used a grid search to identify the best set of parameters. If we did a grid search with this set of parameters with 5-fold cross validation we'd have to run... a lot of models! Instead, we will use a *randomized* parameter search with `RandomizedSearchCV` (__[docs](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)__). \n",
    "\n",
    "The method for setting up a randomized search is very similar to the grid search we've already done. The main adjustment we have to make is that we'll specify how many parameter combinations to try (since we won't try all). We will also set it up to tune each model and optimize on the macro f1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "final_mods = []\n",
    "for mod,params in zip([DTC(),SGD(),RFC(),LGBM()],[dt_params, sgd_params, rf_params, lgbm_params]): # zip creates a list of tuples we can use to iterate\n",
    "    rand_search = RandomizedSearchCV(mod,params, # positional arguments, model and parameter grid\n",
    "                                     n_iter=25,\n",
    "                                     scoring='f1_macro', # see options here: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "                                     cv=5,\n",
    "                                     random_state=123,\n",
    "                                     n_jobs=-1)\n",
    "    \n",
    "    rand_search.fit(trainX,trainy)\n",
    "    print(f\"Classification Report with hold-out sample for best fit of this model:...\\n\")\n",
    "    print(type(mod))\n",
    "    print(classification_report(validy,rand_search.predict(validX.toarray())))\n",
    "    print(\"------------------------------------------------------\")\n",
    "    final_mods.append(rand_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, all of these classifiers really struggle with the bearish tweets. At this point, I would probably conclude this is simply a noisier class, and it's unlikely we can come up with a model that explains them well. \n",
    "\n",
    "However, what if we were in a scenario where getting those right was *much* more important. It may be hard to understand the intuition here, but consider these other scenarios where we have very imbalanced data:\n",
    "1. Understanding which types of narrative disclosures typically appear in intentionally misstated financial statements\n",
    "2. Identifying which types of news paper stories lead to significant litigation\n",
    "3. Understanding which customer complaints result in complete loss of business/revenue.\n",
    "\n",
    "In all of these cases, the base rate of the scenario we are interested in is fairly low (<1% for intentional misstatements).\n",
    "\n",
    "Let's use example 3 above and come up with a custom formula. We will do so by quantifying penalties for each type of error we will encounter:\n",
    "1. Missed 0 (minority class recall): 100 points\n",
    "2. False 0 (minority class precision): 10 points\n",
    "3. True 1 (correctly identify majority class): -1 point\n",
    "\n",
    "We can compute each of these from a confusion matrix and then incorporate the scoring metric into our randomized search. Note that the function we define must \"look\" like traditional scoring metrics, accepting true `y` first and then `y_pred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_scorer(y,y_pred):\n",
    "    cm = confusion_matrix(y,y_pred)\n",
    "    crit1 = cm[0,1] # missed 0s woudl be classified as 1s, so first row 2nd column\n",
    "    crit2 = cm[1,0] # false 0s would be classified as 0s, but actually 1s. so first column 2nd row\n",
    "    crit3 = cm[1,1] # when we get a \"1\" right\n",
    "    return 100*crit1 + 10*crit2 - crit3 # our score\n",
    "\n",
    "# See how our models we just trained did:\n",
    "for mod in final_mods:\n",
    "    print(my_scorer(validy,mod.predict(validX)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before incorporating this into our randomized search, we need to make this function a callable scorer, which `sklearn` accomodates with the `make_scorer` function (__[docs](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html)__). There are a few parameters for this function. The only one we need to adjust is whether a higher score is better (`True` is the default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "myscorer = make_scorer(my_scorer,greater_is_better = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can re-run our search with our new scorer function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_mods2 = []\n",
    "for mod,params in zip([DTC(),SGD(),RFC(),LGBM()],[dt_params, sgd_params, rf_params, lgbm_params]): # zip creates a list of tuples we can use to iterate\n",
    "    rand_search = RandomizedSearchCV(mod,params, # positional arguments, model and parameter grid\n",
    "                                     n_iter=25,\n",
    "                                     scoring=myscorer,\n",
    "                                     cv=5,\n",
    "                                     random_state=123,\n",
    "                                     n_jobs=-1)\n",
    "    \n",
    "    rand_search.fit(trainX,trainy)\n",
    "    print(f\"Classification Report with hold-out sample for best fit of this model:...\\n\")\n",
    "    print(type(mod))\n",
    "    print(classification_report(validy,rand_search.predict(validX.toarray())))\n",
    "    print(\"------------------------------------------------------\")\n",
    "    final_mods2.append(rand_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall model performance declined (by design), but these do appear to do much better with the minority class, at least with respect to recall (which is what had the highest penalty). Let's see what parameters produced the best fit for the LGBM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_mods2[3].best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all we'll cover on NLP-based classifiers in this module. The concepts will come up again when we discuss deep learning, though.\n",
    "\n",
    "Before we conclude, I want to highlight that we did not thoroughly examine all means of optimizing the performance of these models. For instance, we could have:\n",
    "1. Changed the number of features (more or less)\n",
    "2. Applied dimensionality reduction techniques\n",
    "3. Standardized or normalized our data (would impact the non-tree based models)\n",
    "4. Tried resampling (we did not cover this, but oversampling might have improved model performance here by adding synthetic observations to the minority class)\n",
    "\n",
    "**HOMEWORK**\n",
    "I'd like each of you to think about a scenario in your own professional experience where a classifier such as the one we trained may be useful--do you expect class imbalance in your scenario? If so, which class likely bears the greatest cost for misclassification? Discuss with the class on Ed Discussions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:week6]",
   "language": "python",
   "name": "conda-env-week6-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
